{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atomicwrites==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: attrs==19.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 2)) (19.1.0)\n",
      "Requirement already satisfied: certifi==2019.3.9 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: chardet==3.0.4 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: codecov==2.0.15 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 5)) (2.0.15)\n",
      "Requirement already satisfied: colorama==0.4.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: coverage==4.5.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 7)) (4.5.3)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: idna==2.8 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: imbalanced-learn==0.4.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: imblearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 11)) (0.0)\n",
      "Requirement already satisfied: importlib-metadata==0.17 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 12)) (0.17)\n",
      "Requirement already satisfied: joblib==0.13.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 13)) (0.13.2)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: matplotlib==3.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: more-itertools==7.0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 16)) (7.0.0)\n",
      "Requirement already satisfied: nltk==3.4.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 17)) (3.4.1)\n",
      "Requirement already satisfied: numpy==1.16.4 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 18)) (1.16.4)\n",
      "Requirement already satisfied: packaging==19.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 19)) (19.0)\n",
      "Requirement already satisfied: pandas==0.24.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 20)) (0.24.2)\n",
      "Requirement already satisfied: patsy==0.5.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (0.5.1)\n",
      "Requirement already satisfied: pluggy==0.12.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 22)) (0.12.0)\n",
      "Requirement already satisfied: py==1.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 23)) (1.8.0)\n",
      "Requirement already satisfied: pyparsing==2.4.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 24)) (2.4.0)\n",
      "Requirement already satisfied: pytest==4.6.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 25)) (4.6.0)\n",
      "Requirement already satisfied: pytest-cov==2.7.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 26)) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 27)) (2.8.0)\n",
      "Requirement already satisfied: pytz==2019.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 28)) (2019.1)\n",
      "Requirement already satisfied: requests==2.22.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 29)) (2.22.0)\n",
      "Requirement already satisfied: saxpy==1.0.1.dev167 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 30)) (1.0.1.dev167)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 31)) (0.21.2)\n",
      "Requirement already satisfied: scipy==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 32)) (1.3.0)\n",
      "Requirement already satisfied: seaborn==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 33)) (0.9.0)\n",
      "Requirement already satisfied: six==1.12.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 34)) (1.12.0)\n",
      "Requirement already satisfied: sklearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 35)) (0.0)\n",
      "Requirement already satisfied: statsmodels==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 36)) (0.9.0)\n",
      "Requirement already satisfied: urllib3==1.25.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 37)) (1.25.3)\n",
      "Requirement already satisfied: wcwidth==0.1.7 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 38)) (0.1.7)\n",
      "Requirement already satisfied: zipp==0.5.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 39)) (0.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from kiwisolver==1.1.0->-r requirements.txt (line 14)) (40.6.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BClus(df):\n",
    "    \"\"\"\n",
    "    Function that splits the netflow data into time windows and then aggregates them by source ip into aggregation\n",
    "    windows in order for the BClus dataset to be constructed\n",
    "    :param df: the initial dataframe\n",
    "    :return: the BClus dataframe\n",
    "    \"\"\"\n",
    "    t_start = df.index[0]\n",
    "    new_data = pd.DataFrame()\n",
    "    while t_start in df.index:\n",
    "        # take a time window of 2 minutes\n",
    "        t_end = t_start + datetime.timedelta(minutes=2)\n",
    "        window = df.loc[(df.index >= t_start) & (df.index <= t_end)]\n",
    "\n",
    "        # keep the remaining data\n",
    "        remaining = df.loc[df.index > t_end]\n",
    "\n",
    "        # loop for inner aggregation window\n",
    "        agg_start = t_start\n",
    "        for i in range(2):\n",
    "            agg_end = agg_start + datetime.timedelta(minutes=1)\n",
    "            agg_window = window.loc[(window.index >= agg_start) & (window.index <= agg_end)]\n",
    "\n",
    "            # aggregate the data by source IP address\n",
    "            src_groups = agg_window.groupby('src_ip')\n",
    "            aggr = src_groups.aggregate({'packets': np.sum, 'bytes': np.sum, 'flows': np.sum})\n",
    "            aggr['dst_ips'] = agg_window.groupby('src_ip').dst_ip.nunique()\n",
    "            aggr['src_ports'] = agg_window.groupby('src_ip').src_port.nunique()\n",
    "            aggr['dst_ports'] = agg_window.groupby('src_ip').dst_port.nunique()\n",
    "\n",
    "            # and add them to the new dataset\n",
    "            new_data = new_data.append(aggr, ignore_index=False)\n",
    "            agg_start = agg_end\n",
    "\n",
    "        if not len(remaining):\n",
    "            break\n",
    "        else:\n",
    "            t_start = remaining.index[0]\n",
    "\n",
    "    new_data = new_data.reset_index()\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_infected(val, infected_ips):\n",
    "    \"\"\"\n",
    "    Function to assign labels to instances given their ip\n",
    "    :param val: the flow to be checked\n",
    "    :param infected_ips: the list of infected hosts\n",
    "    :return: 1 if val is in the infected list otherwise 0\n",
    "    \"\"\"\n",
    "    return 1 if val in infected_ips else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf_cross(usx, usy, clf, clf_name, level):\n",
    "    \"\"\"\n",
    "    Function for the classification task - Trains and tests the classifier clf using 10-fold cross-validation\n",
    "    The sampling parameter sets the type of sampling to be used\n",
    "    :param usx: the input instances\n",
    "    :param usy: the labels of the instances\n",
    "    :param clf: the classifier to be used\n",
    "    :param clf_name: the name of the classifier (for plotting reasons)\n",
    "    :param level: the evaluation level (for plotting reasons)\n",
    "    :return: the classification results\n",
    "    \"\"\"\n",
    "    print('---------- {} at {} level ----------'.format(clf_name, level))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    j = 0\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)  # apply 10-fold stratified cross validation\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "\n",
    "        # split data in training and test set\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "\n",
    "        # apply SMOTE for imbalance issues\n",
    "        x_train, y_train = SMOTE(sampling_strategy=0.5).fit_resample(x_train, y_train)\n",
    "\n",
    "        # create_clusters(x_train, y_train, train_ips)  # TODO: not fully implemented yet - decisions still to be made\n",
    "\n",
    "        # fit the model and make predictions\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_predict = clf.predict(x_test)\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] and y_predict[i]:\n",
    "                totalTP += 1\n",
    "            if not y_test[i] and y_predict[i]:\n",
    "                totalFP += 1\n",
    "            if y_test[i] and not y_predict[i]:\n",
    "                totalFN += 1\n",
    "            if not y_test[i] and not y_predict[i]:\n",
    "                totalTN += 1\n",
    "        j += 1\n",
    "\n",
    "    # just in case that no TP or FP are found\n",
    "    if not (totalTP + totalFP):\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = totalTP / (totalTP + totalFP)\n",
    "    recall = totalTP / (totalTP + totalFN)\n",
    "    accuracy = (totalTP + totalTN) / (totalTP + totalFN + totalTN + totalFP)\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))\n",
    "    print('TOTAL Accuracy: ' + str(accuracy))\n",
    "    print('TOTAL Precision: ' + str(precision))\n",
    "    print('TOTAL Recall: ' + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the scenario 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data without the background are there, load them (again data from scenario 10 were used)\n",
    "data = pd.read_pickle('no_background_data.pkl')\n",
    "\n",
    "# resetting indices for data\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# parse packets and bytes as integers instead of strings\n",
    "data['packets'] = data['packets'].astype(int)\n",
    "data['bytes'] = data['bytes'].astype(int)\n",
    "\n",
    "# set date as index in the dataframe\n",
    "data = data.set_index(data.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BClus dataset from the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BClus dataset\n",
    "bclus_data = create_BClus(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set the classifier to be used and the level of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the classifiers\n",
    "clf_name = 'RandomForestClassifier'\n",
    "clf = RandomForestClassifier(n_estimators=50, criterion='gini', class_weight='balanced')\n",
    "\n",
    "# name the infected hosts\n",
    "infected_ips = ['147.32.84.165', '147.32.84.191', '147.32.84.192', '147.32.84.193', '147.32.84.204',\n",
    "                '147.32.84.205', '147.32.84.206', '147.32.84.207', '147.32.84.208', '147.32.84.209']\n",
    "\n",
    "# enter the classification phase for each level\n",
    "eval_levels = ['packet', 'host']  # the 2 evaluation levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform classification for both levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the classification process\n",
      "---------- RandomForestClassifier at packet level ----------\n",
      "TOTAL TP: 1061\n",
      "TOTAL FP: 93\n",
      "TOTAL FN: 67\n",
      "TOTAL TN: 20556\n",
      "TOTAL Accuracy: 0.9926527988244478\n",
      "TOTAL Precision: 0.9194107452339688\n",
      "TOTAL Recall: 0.9406028368794326\n",
      "Start the classification process\n",
      "---------- RandomForestClassifier at host level ----------\n",
      "TOTAL TP: 10\n",
      "TOTAL FP: 1\n",
      "TOTAL FN: 0\n",
      "TOTAL TN: 511\n",
      "TOTAL Accuracy: 0.9980842911877394\n",
      "TOTAL Precision: 0.9090909090909091\n",
      "TOTAL Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "for level in eval_levels:\n",
    "    # prepare the data according to the level\n",
    "    final_data = bclus_data.copy()\n",
    "\n",
    "    if level == 'host':\n",
    "        final_data = final_data.groupby('src_ip').sum().reset_index()\n",
    "\n",
    "    # label the processed dataset(s)\n",
    "    final_data['label'] = final_data['src_ip'].apply(lambda z: check_infected(z, infected_ips))\n",
    "\n",
    "    # separate the labels from the rest of the dataset\n",
    "    y = final_data['label'].values\n",
    "    x = final_data.drop(['src_ip', 'label'], axis=1).values\n",
    "\n",
    "    # enter the classification phase\n",
    "    print('Start the classification process')\n",
    "    usx = np.copy(x)\n",
    "    usy = np.copy(y)\n",
    "    make_clf_cross(usx, usy, clf, clf_name, level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
