{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from classification import create_BClus, check_infected\n",
    "from discretization import find_percentile, netflow_encoding\n",
    "from profiling import fit_and_apply_hmm, classify\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed lists for the creation of adversarial datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usage of these perturbation steps and types are inspired from Apruzzese, Giovanni, and Michele Colajanni. \n",
    "# \"Evading Botnet Detectors Based on Flows and Random Forest with Adversarial Samples.\" 2018 IEEE 17th International \n",
    "# Symposium on Network Computing and Applications (NCA). IEEE, 2018.\n",
    "perturbation_steps = {\n",
    "    1: {'packets': 1, 'bytes': 1},\n",
    "    2: {'packets': 10, 'bytes': 16},\n",
    "    3: {'packets': 15, 'bytes': 64},\n",
    "    4: {'packets': 30, 'bytes': 256},\n",
    "    5: {'packets': 100, 'bytes': 1024}\n",
    "}\n",
    "\n",
    "perturbation_types = {\n",
    "    1: ['packets'],\n",
    "    2: ['bytes'],\n",
    "    3: ['packets', 'bytes'],\n",
    "}\n",
    "\n",
    "# name the infected hosts\n",
    "infected_ips = ['147.32.84.165', '147.32.84.191', '147.32.84.192', '147.32.84.193', '147.32.84.204',\n",
    "                '147.32.84.205', '147.32.84.206', '147.32.84.207', '147.32.84.208', '147.32.84.209']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to create adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adversarial(df, altered):\n",
    "    \"\"\"\n",
    "    Function that creates an adversarial dataset in the following way. The botnet flows are altered by adding packets\n",
    "    or (and) bytes according to the perturbation types and steps specified as input\n",
    "    :param df: the dataframe of the initial dataset\n",
    "    :param altered: the features to be altered\n",
    "    :return: the new adversarial dataset\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    botnets = new_df[new_df['label'] == 'Botnet']  # keep the botnet flows\n",
    "    new_df = new_df[new_df['label'] != 'Botnet']  # and remove them from the original dataset\n",
    "\n",
    "    # alter the packets or (and) the bytes according ot the values of the altered dictionary\n",
    "    botnets['packets'] = botnets['packets'].apply(lambda z: z + (0 if 'packets' not in altered.keys() else altered['packets']))\n",
    "    botnets['bytes'] = botnets['bytes'].apply(lambda z: z + (0 if 'bytes' not in altered.keys() else altered['bytes']))\n",
    "\n",
    "    # and concatenate the new botnet flows with the original dataset with the original dataset\n",
    "    fin_df = pd.concat([new_df, botnets])\n",
    "    return fin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf(x_train, y_train, x_test, y_test, clf, clf_name, level):\n",
    "    \"\"\"\n",
    "    Function mostly implemented for the adversarial task - Trains and tests the classifier clf using the initial dataset\n",
    "    as training set and the adversarial dataset as test set\n",
    "    The sampling parameter sets the type of sampling to be used\n",
    "    :param x_train: the original dataset\n",
    "    :param y_train: the labels of the instances in the original dataset\n",
    "    :param x_test: the adversarial test set\n",
    "    :param y_test: the labels of the instances in the adversarial dataset\n",
    "    :param clf: the classifier to be used\n",
    "    :param clf_name: the name of the classifier (for plotting reasons)\n",
    "    :param level: the evaluation level (for plotting reasons)\n",
    "    :return: the classification results\n",
    "    \"\"\"\n",
    "    print('----------{} at {} level ----------'.format(clf_name, level))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "\n",
    "    # apply SMOTE, train and test the model\n",
    "    x_train, y_train = SMOTE(sampling_strategy=0.5).fit_resample(x_train, y_train)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_predict = clf.predict(x_test)\n",
    "\n",
    "    for i in range(len(y_predict)):\n",
    "        if y_test[i] and y_predict[i]:\n",
    "            totalTP += 1\n",
    "        if not y_test[i] and y_predict[i]:\n",
    "            totalFP += 1\n",
    "        if y_test[i] and not y_predict[i]:\n",
    "            totalFN += 1\n",
    "        if not y_test[i] and not y_predict[i]:\n",
    "            totalTN += 1\n",
    "\n",
    "    recall = totalTP / (totalTP + totalFN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the initial scenario 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data without the background are there, load them (again data from scenario 10 were used)\n",
    "data = pd.read_pickle('no_background_data.pkl')\n",
    "\n",
    "# resetting indices for data\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# parse packets and bytes as integers instead of strings\n",
    "data['packets'] = data['packets'].astype(int)\n",
    "data['bytes'] = data['bytes'].astype(int)\n",
    "\n",
    "# sort data by date just to be sure that flows are in chronological order\n",
    "data.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# set date as index in the dataframe\n",
    "data = data.set_index(data.date)\n",
    "data.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BClus training dataset to be used for classification and set the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bclus_data = create_BClus(data)\n",
    "\n",
    "# set the classifiers\n",
    "clf_name = 'RandomForestClassifier'\n",
    "clf = RandomForestClassifier(n_estimators=50, criterion='gini', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test the new adversarial datasets \n",
    "### (it needs some time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating perturbation type 1 with step 1...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 1 with step 2...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 1 with step 3...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 1 with step 4...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 1 with step 5...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 2 with step 1...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 2 with step 2...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 2 with step 3...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 2 with step 4...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 2 with step 5...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 3 with step 1...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 3 with step 2...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 3 with step 3...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 3 with step 4...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n",
      "Creating perturbation type 3 with step 5...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n",
      "Profiling in process...\n"
     ]
    }
   ],
   "source": [
    "# create all the adversarial datasets\n",
    "results_classification_packet = []\n",
    "results_classification_host = []\n",
    "results_profiling = []\n",
    "for types in perturbation_types.keys():\n",
    "    step_results_classification_packet = []\n",
    "    step_results_classification_host = []\n",
    "    step_results_profiling = []\n",
    "    for step in perturbation_steps.keys():\n",
    "        print('Creating perturbation type %d with step %d...' % (types, step))\n",
    "        to_be_altered = {}\n",
    "        for type in perturbation_types[types]:\n",
    "            to_be_altered[type] = perturbation_steps[step][type]\n",
    "        adv_df = make_adversarial(data, to_be_altered)\n",
    "\n",
    "        # save them just in case\n",
    "        adv_df.to_pickle('adversarial_examples/altered_%s_step_%d.pkl' % ('_'.join(perturbation_types[types]), step))\n",
    "\n",
    "        print('Applying flow classification...')\n",
    "        # Create BClus test dataset with the adversarial dataset\n",
    "        adv_df['packets'] = adv_df['packets'].astype(int)\n",
    "        adv_df['bytes'] = adv_df['bytes'].astype(int)\n",
    "        bclus_test_data = create_BClus(adv_df)\n",
    "\n",
    "        # enter the classification phase for each level\n",
    "        eval_levels = ['packet', 'host']  # the 2 evaluation levels\n",
    "        for level in eval_levels:\n",
    "            # prepare the data according to the level\n",
    "            final_data = bclus_data.copy()\n",
    "            final_test_data = bclus_test_data.copy()\n",
    "\n",
    "            if level == 'host':\n",
    "                final_data = final_data.groupby('src_ip').sum().reset_index()\n",
    "                final_test_data = final_test_data.groupby('src_ip').sum().reset_index()\n",
    "\n",
    "            # label the processed datasets\n",
    "            final_data['label'] = final_data['src_ip'].apply(lambda z: check_infected(z, infected_ips))\n",
    "            final_test_data['label'] = final_test_data['src_ip'].apply(lambda z: check_infected(z, infected_ips))\n",
    "\n",
    "            # separate the labels from the rest of the dataset\n",
    "            y = final_data['label'].values\n",
    "            x = final_data.drop(['src_ip', 'label'], axis=1).values\n",
    "\n",
    "            y_test = final_test_data['label'].values\n",
    "            x_test = final_test_data.drop(['src_ip', 'label'], axis=1).values\n",
    "\n",
    "            # enter the classification phase\n",
    "            print('Start the classification process')\n",
    "            usx = np.copy(x)\n",
    "            usy = np.copy(y)\n",
    "            usx_test = np.copy(x_test)\n",
    "            usy_test = np.copy(y_test)\n",
    "            recall = make_clf(usx, usy, usx_test, usy_test, clf, clf_name, level)\n",
    "\n",
    "            # store the results\n",
    "            if level == 'packet':\n",
    "                step_results_classification_packet += [recall]\n",
    "            else:\n",
    "                step_results_classification_host += [recall]\n",
    "\n",
    "        print('Discretizing data for the profiling task...')\n",
    "        \n",
    "        # add the numerical representation of the categorical data\n",
    "        adv_df['protocol_num'] = pd.Categorical(adv_df['protocol'], categories=adv_df['protocol'].unique()).codes\n",
    "        adv_df['flags_num'] = pd.Categorical(adv_df['flags'], categories=adv_df['flags'].unique()).codes\n",
    "        \n",
    "        # pick one infected host and the normal ones\n",
    "        infected_ip = '147.32.84.165'\n",
    "        normal_ips = ['147.32.84.170', '147.32.84.134', '147.32.84.164', '147.32.87.36', '147.32.80.9',\n",
    "                      '147.32.87.11']\n",
    "\n",
    "        # list the continuous types of features in the dataset\n",
    "        continuous_features = ['duration', 'protocol_num', 'flags_num', 'tos', 'packets', 'bytes', 'flows']\n",
    "\n",
    "        # and select the features that were selected for the profiling task with same number of bins as before\n",
    "        selected_features = ['protocol', 'bytes']\n",
    "        for sel in selected_features:\n",
    "            if sel in continuous_features:\n",
    "                percentile_num = 4\n",
    "                # assign the cluster id to each value of the selected numerical feature in the way that it is\n",
    "                # described in Pellegrino, Gaetano, et al. \"Learning Behavioral Fingerprints From Netflows Using\n",
    "                # Timed Automata.\"\n",
    "                percentile_values = list(\n",
    "                    map(lambda p: np.percentile(adv_df[sel], p), 100 * np.arange(0, 1, 1 / percentile_num)[1:]))\n",
    "                adv_df[sel + '_num'] = adv_df[sel].apply(find_percentile, args=(percentile_values,))\n",
    "\n",
    "        # discretize all flows\n",
    "        print('Discretizing all hosts...')\n",
    "        mappings = {}\n",
    "        for sel_feat in selected_features:\n",
    "            mappings[sel_feat] = len(adv_df[sel_feat + '_num'].unique())\n",
    "        adv_df['encoded'] = adv_df.apply(lambda x: netflow_encoding(x, mappings), axis=1)\n",
    "\n",
    "        # proceed to profiling\n",
    "        print('Profiling in process...')\n",
    "        chosen = adv_df[(adv_df['src_ip'] == infected_ip) | (adv_df['dst_ip'] == infected_ip)]\n",
    "        hosts_log_likelihood, modeled_log_likelihood = fit_and_apply_hmm(normal_ips, infected_ips, chosen, adv_df)\n",
    "        recall, _, _, _, _, _, _ = classify(hosts_log_likelihood, normal_ips, infected_ips, modeled_log_likelihood)\n",
    "        step_results_profiling += [recall]\n",
    "\n",
    "    results_classification_packet += [step_results_classification_packet]\n",
    "    results_classification_host += [step_results_classification_host]\n",
    "    results_profiling += [step_results_profiling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results \n",
    "### (Rows correspond to the perturbation types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Flow classification results for packet level ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step 1</th>\n",
       "      <th>step 2</th>\n",
       "      <th>step 3</th>\n",
       "      <th>step 4</th>\n",
       "      <th>step 5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[packets]</th>\n",
       "      <td>0.656584</td>\n",
       "      <td>0.835409</td>\n",
       "      <td>0.864769</td>\n",
       "      <td>0.870996</td>\n",
       "      <td>0.875445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[bytes]</th>\n",
       "      <td>0.971530</td>\n",
       "      <td>0.643238</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.586299</td>\n",
       "      <td>0.586299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[packets, bytes]</th>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>0.575623</td>\n",
       "      <td>0.553381</td>\n",
       "      <td>0.548043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store the final results into dataframe for better visualization and print them\n",
    "headers = ['types', 'step 1', 'step 2', 'step 3', 'step 4', 'step 5']\n",
    "\n",
    "results_classification_packet = [[prt] + row for row, prt in\n",
    "                                     zip(results_classification_packet, list(perturbation_types.values()))]\n",
    "results_classification_packet_df = pd.DataFrame(results_classification_packet, columns=headers)\n",
    "results_classification_packet_df.set_index('types', inplace=True)\n",
    "print('--------- Flow classification results for packet level ---------')\n",
    "display(HTML(results_classification_packet_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Flow classification results for host level ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step 1</th>\n",
       "      <th>step 2</th>\n",
       "      <th>step 3</th>\n",
       "      <th>step 4</th>\n",
       "      <th>step 5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[packets]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[bytes]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[packets, bytes]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_classification_host = [[prt] + row for row, prt in\n",
    "                                     zip(results_classification_host, list(perturbation_types.values()))]\n",
    "results_classification_host_df = pd.DataFrame(results_classification_host, columns=headers)\n",
    "results_classification_host_df.set_index('types', inplace=True)\n",
    "print('--------- Flow classification results for host level ---------')\n",
    "display(HTML(results_classification_host_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Profiling results ---------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step 1</th>\n",
       "      <th>step 2</th>\n",
       "      <th>step 3</th>\n",
       "      <th>step 4</th>\n",
       "      <th>step 5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[packets]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[bytes]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[packets, bytes]</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_profiling = [[prt] + row for row, prt in zip(results_profiling, list(perturbation_types.values()))]\n",
    "results_profiling_df = pd.DataFrame(results_profiling, columns=headers)\n",
    "results_profiling_df.set_index('types', inplace=True)\n",
    "print('--------- Profiling results ---------')\n",
    "display(HTML(results_profiling_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
