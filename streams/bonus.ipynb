{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atomicwrites==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: attrs==19.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 2)) (19.1.0)\n",
      "Requirement already satisfied: certifi==2019.3.9 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: chardet==3.0.4 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: codecov==2.0.15 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 5)) (2.0.15)\n",
      "Requirement already satisfied: colorama==0.4.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: coverage==4.5.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 7)) (4.5.3)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: idna==2.8 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: imbalanced-learn==0.4.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: imblearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 11)) (0.0)\n",
      "Requirement already satisfied: importlib-metadata==0.17 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 12)) (0.17)\n",
      "Requirement already satisfied: joblib==0.13.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 13)) (0.13.2)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: matplotlib==3.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: more-itertools==7.0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 16)) (7.0.0)\n",
      "Requirement already satisfied: nltk==3.4.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 17)) (3.4.1)\n",
      "Requirement already satisfied: numpy==1.16.4 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 18)) (1.16.4)\n",
      "Requirement already satisfied: packaging==19.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 19)) (19.0)\n",
      "Requirement already satisfied: pandas==0.24.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 20)) (0.24.2)\n",
      "Requirement already satisfied: patsy==0.5.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (0.5.1)\n",
      "Requirement already satisfied: pluggy==0.12.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 22)) (0.12.0)\n",
      "Requirement already satisfied: py==1.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 23)) (1.8.0)\n",
      "Requirement already satisfied: pyparsing==2.4.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 24)) (2.4.0)\n",
      "Requirement already satisfied: pytest==4.6.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 25)) (4.6.0)\n",
      "Requirement already satisfied: pytest-cov==2.7.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 26)) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 27)) (2.8.0)\n",
      "Requirement already satisfied: pytz==2019.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 28)) (2019.1)\n",
      "Requirement already satisfied: requests==2.22.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 29)) (2.22.0)\n",
      "Requirement already satisfied: saxpy==1.0.1.dev167 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 30)) (1.0.1.dev167)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 31)) (0.21.2)\n",
      "Requirement already satisfied: scipy==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 32)) (1.3.0)\n",
      "Requirement already satisfied: seaborn==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 33)) (0.9.0)\n",
      "Requirement already satisfied: six==1.12.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 34)) (1.12.0)\n",
      "Requirement already satisfied: sklearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 35)) (0.0)\n",
      "Requirement already satisfied: statsmodels==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 36)) (0.9.0)\n",
      "Requirement already satisfied: urllib3==1.25.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 37)) (1.25.3)\n",
      "Requirement already satisfied: wcwidth==0.1.7 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 38)) (0.1.7)\n",
      "Requirement already satisfied: zipp==0.5.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 39)) (0.5.1)\n",
      "Requirement already satisfied: hmmlearn==0.2.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 40)) (0.2.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from kiwisolver==1.1.0->-r requirements.txt (line 14)) (40.6.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from classification import create_BClus, check_infected\n",
    "from discretization import find_percentile, netflow_encoding\n",
    "from profiling import fit_and_apply_hmm, classify\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed lists for the creation of adversarial datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usage of these perturbation steps and types are inspired from Apruzzese, Giovanni, and Michele Colajanni. \n",
    "# \"Evading Botnet Detectors Based on Flows and Random Forest with Adversarial Samples.\" 2018 IEEE 17th International \n",
    "# Symposium on Network Computing and Applications (NCA). IEEE, 2018.\n",
    "perturbation_steps = {\n",
    "    1: {'packets': 1, 'bytes': 1},\n",
    "    2: {'packets': 10, 'bytes': 16},\n",
    "    3: {'packets': 15, 'bytes': 64},\n",
    "    4: {'packets': 30, 'bytes': 256},\n",
    "    5: {'packets': 100, 'bytes': 1024}\n",
    "}\n",
    "\n",
    "perturbation_types = {\n",
    "    1: ['packets'],\n",
    "    2: ['bytes'],\n",
    "    3: ['packets', 'bytes'],\n",
    "}\n",
    "\n",
    "# name the infected hosts\n",
    "infected_ips = ['147.32.84.165', '147.32.84.191', '147.32.84.192', '147.32.84.193', '147.32.84.204',\n",
    "                '147.32.84.205', '147.32.84.206', '147.32.84.207', '147.32.84.208', '147.32.84.209']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to create adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adversarial(df, altered):\n",
    "    \"\"\"\n",
    "    Function that creates an adversarial dataset in the following way. The botnet flows are altered by adding packets\n",
    "    or (and) bytes according to the perturbation types and steps specified as input\n",
    "    :param df: the dataframe of the initial dataset\n",
    "    :param altered: the features to be altered\n",
    "    :return: the new adversarial dataset\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    botnets = new_df[new_df['label'] == 'Botnet']  # keep the botnet flows\n",
    "    new_df = new_df[new_df['label'] != 'Botnet']  # and remove them from the original dataset\n",
    "\n",
    "    # alter the packets or (and) the bytes according ot the values of the altered dictionary\n",
    "    botnets['packets'] = botnets['packets'].apply(lambda z: z + (0 if 'packets' not in altered.keys() else altered['packets']))\n",
    "    botnets['bytes'] = botnets['bytes'].apply(lambda z: z + (0 if 'bytes' not in altered.keys() else altered['bytes']))\n",
    "\n",
    "    # and concatenate the new botnet flows with the original dataset with the original dataset\n",
    "    fin_df = pd.concat([new_df, botnets])\n",
    "    return fin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf(x_train, y_train, x_test, y_test, clf, clf_name, level):\n",
    "    \"\"\"\n",
    "    Function mostly implemented for the adversarial task - Trains and tests the classifier clf using the initial dataset\n",
    "    as training set and the adversarial dataset as test set\n",
    "    The sampling parameter sets the type of sampling to be used\n",
    "    :param x_train: the original dataset\n",
    "    :param y_train: the labels of the instances in the original dataset\n",
    "    :param x_test: the adversarial test set\n",
    "    :param y_test: the labels of the instances in the adversarial dataset\n",
    "    :param clf: the classifier to be used\n",
    "    :param clf_name: the name of the classifier (for plotting reasons)\n",
    "    :param level: the evaluation level (for plotting reasons)\n",
    "    :return: the classification results\n",
    "    \"\"\"\n",
    "    print('----------{} at {} level ----------'.format(clf_name, level))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "\n",
    "    # apply SMOTE, train and test the model\n",
    "    x_train, y_train = SMOTE(sampling_strategy=0.5).fit_resample(x_train, y_train)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_predict = clf.predict(x_test)\n",
    "\n",
    "    for i in range(len(y_predict)):\n",
    "        if y_test[i] and y_predict[i]:\n",
    "            totalTP += 1\n",
    "        if not y_test[i] and y_predict[i]:\n",
    "            totalFP += 1\n",
    "        if y_test[i] and not y_predict[i]:\n",
    "            totalFN += 1\n",
    "        if not y_test[i] and not y_predict[i]:\n",
    "            totalTN += 1\n",
    "\n",
    "    recall = totalTP / (totalTP + totalFN)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the initial scenario 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data without the background are there, load them (again data from scenario 10 were used)\n",
    "data = pd.read_pickle('no_background_data.pkl')\n",
    "\n",
    "# resetting indices for data\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# parse packets and bytes as integers instead of strings\n",
    "data['packets'] = data['packets'].astype(int)\n",
    "data['bytes'] = data['bytes'].astype(int)\n",
    "\n",
    "# sort data by date just to be sure that flows are in chronological order\n",
    "data.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# set date as index in the dataframe\n",
    "data = data.set_index(data.date)\n",
    "data.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BClus training dataset to be used for classification and set the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bclus_data = create_BClus(data)\n",
    "\n",
    "# set the classifiers\n",
    "clf_name = 'RandomForestClassifier'\n",
    "clf = RandomForestClassifier(n_estimators=50, criterion='gini', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test the new adversarial datasets \n",
    "### (it needs some time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating perturbation type 1 with step 1...\n",
      "Applying flow classification...\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at packet level ----------\n",
      "Start the classification process\n",
      "----------RandomForestClassifier at host level ----------\n",
      "Discretizing data for the profiling task...\n",
      "Discretizing all hosts...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c336348e429b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mchosen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madv_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src_ip'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minfected_ip\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0madv_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dst_ip'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minfected_ip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mhosts_log_likelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodeled_log_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_and_apply_hmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchosen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhosts_log_likelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodeled_log_likelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mstep_results_profiling\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\cyber_data_analytics\\streams\\profiling.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(hosts_log_likelihood, normal, infected, modeled_log_likelihood)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'True Positives : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# create all the adversarial datasets\n",
    "results_classification_packet = []\n",
    "results_classification_host = []\n",
    "results_profiling = []\n",
    "for types in perturbation_types.keys():\n",
    "    step_results_classification_packet = []\n",
    "    step_results_classification_host = []\n",
    "    step_results_profiling = []\n",
    "    for step in perturbation_steps.keys():\n",
    "        print('Creating perturbation type %d with step %d...' % (types, step))\n",
    "        to_be_altered = {}\n",
    "        for type in perturbation_types[types]:\n",
    "            to_be_altered[type] = perturbation_steps[step][type]\n",
    "        adv_df = make_adversarial(data, to_be_altered)\n",
    "\n",
    "        # save them just in case\n",
    "        adv_df.to_pickle('adversarial_examples/altered_%s_step_%d.pkl' % ('_'.join(perturbation_types[types]), step))\n",
    "\n",
    "        print('Applying flow classification...')\n",
    "        # Create BClus test dataset with the adversarial dataset\n",
    "        adv_df['packets'] = adv_df['packets'].astype(int)\n",
    "        adv_df['bytes'] = adv_df['bytes'].astype(int)\n",
    "        bclus_test_data = create_BClus(adv_df)\n",
    "\n",
    "        # enter the classification phase for each level\n",
    "        eval_levels = ['packet', 'host']  # the 2 evaluation levels\n",
    "        for level in eval_levels:\n",
    "            # prepare the data according to the level\n",
    "            final_data = bclus_data.copy()\n",
    "            final_test_data = bclus_test_data.copy()\n",
    "\n",
    "            if level == 'host':\n",
    "                final_data = final_data.groupby('src_ip').sum().reset_index()\n",
    "                final_test_data = final_test_data.groupby('src_ip').sum().reset_index()\n",
    "\n",
    "            # label the processed datasets\n",
    "            final_data['label'] = final_data['src_ip'].apply(lambda z: check_infected(z, infected_ips))\n",
    "            final_test_data['label'] = final_test_data['src_ip'].apply(lambda z: check_infected(z, infected_ips))\n",
    "\n",
    "            # separate the labels from the rest of the dataset\n",
    "            y = final_data['label'].values\n",
    "            x = final_data.drop(['src_ip', 'label'], axis=1).values\n",
    "\n",
    "            y_test = final_test_data['label'].values\n",
    "            x_test = final_test_data.drop(['src_ip', 'label'], axis=1).values\n",
    "\n",
    "            # enter the classification phase\n",
    "            print('Start the classification process')\n",
    "            usx = np.copy(x)\n",
    "            usy = np.copy(y)\n",
    "            usx_test = np.copy(x_test)\n",
    "            usy_test = np.copy(y_test)\n",
    "            recall = make_clf(usx, usy, usx_test, usy_test, clf, clf_name, level)\n",
    "\n",
    "            # store the results\n",
    "            if level == 'packet':\n",
    "                step_results_classification_packet += [recall]\n",
    "            else:\n",
    "                step_results_classification_host += [recall]\n",
    "\n",
    "        print('Discretizing data for the profiling task...')\n",
    "        \n",
    "        # add the numerical representation of the categorical data\n",
    "        adv_df['protocol_num'] = pd.Categorical(adv_df['protocol'], categories=adv_df['protocol'].unique()).codes\n",
    "        adv_df['flags_num'] = pd.Categorical(adv_df['flags'], categories=adv_df['flags'].unique()).codes\n",
    "        \n",
    "        # pick one infected host and the normal ones\n",
    "        infected_ip = '147.32.84.165'\n",
    "        normal_ips = ['147.32.84.170', '147.32.84.134', '147.32.84.164', '147.32.87.36', '147.32.80.9',\n",
    "                      '147.32.87.11']\n",
    "\n",
    "        # currently using only source ips for infected and normal discrimination\n",
    "        infected = adv_df[adv_df['src_ip'] == infected_ip]\n",
    "        infected = infected.reset_index()\n",
    "\n",
    "        normal = adv_df[adv_df['src_ip'].isin(normal_ips)]\n",
    "        normal = normal.reset_index()\n",
    "\n",
    "        # separate the types of features in the dataset\n",
    "        continuous_features = ['duration', 'protocol_num', 'flags_num', 'tos', 'packets', 'bytes', 'flows']\n",
    "        categorical_features = ['protocol', 'flags']\n",
    "\n",
    "        # and select the features that were selected for the profiling task with same number of bins as before\n",
    "        selected_features = ['protocol', 'bytes']\n",
    "        for sel in selected_features:\n",
    "            if sel in continuous_features:\n",
    "                percentile_num = 4\n",
    "                # assign the cluster id to each value of the selected numerical feature in the way that it is\n",
    "                # described in Pellegrino, Gaetano, et al. \"Learning Behavioral Fingerprints From Netflows Using\n",
    "                # Timed Automata.\"\n",
    "                percentile_values = list(\n",
    "                    map(lambda p: np.percentile(adv_df[sel], p), 100 * np.arange(0, 1, 1 / percentile_num)[1:]))\n",
    "                adv_df[sel + '_num'] = adv_df[sel].apply(find_percentile, args=(percentile_values,))\n",
    "\n",
    "        # discretize all flows\n",
    "        print('Discretizing all hosts...')\n",
    "        mappings = {}\n",
    "        for sel_feat in selected_features:\n",
    "            mappings[sel_feat] = len(adv_df[sel_feat + '_num'].unique())\n",
    "        adv_df['encoded'] = adv_df.apply(lambda x: netflow_encoding(x, mappings), axis=1)\n",
    "\n",
    "        # proceed to profiling\n",
    "        print('Profiling in process...')\n",
    "        chosen = adv_df[(adv_df['src_ip'] == infected_ip) | (adv_df['dst_ip'] == infected_ip)]\n",
    "        hosts_log_likelihood, modeled_log_likelihood = fit_and_apply_hmm(normal_ips, infected_ips, chosen, adv_df)\n",
    "        recall = classify(hosts_log_likelihood, normal_ips, infected_ips, modeled_log_likelihood)\n",
    "        step_results_profiling += [recall]\n",
    "\n",
    "    results_classification_packet += [step_results_classification_packet]\n",
    "    results_classification_host = [step_results_classification_host]\n",
    "    results_profiling = [step_results_profiling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the results \n",
    "### (Rows correspond to the perturbation types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final results into dataframe for better visualization and print them\n",
    "headers = ['step 1', 'step 2', 'step 3', 'step 4', 'step 5']\n",
    "\n",
    "results_classification_packet_df = pd.DataFrame(results_classification_packet, columns=headers)\n",
    "print('--------- Flow classification results for packet level ---------')\n",
    "display(HTML(results_classification_packet_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_classification_host_df = pd.DataFrame(results_classification_host, columns=headers)\n",
    "print('--------- Flow classification results for host level ---------')\n",
    "display(HTML(results_classification_host_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_profiling_df = pd.DataFrame(results_profiling, columns=headers)\n",
    "print('--------- Profiling results ---------')\n",
    "display(HTML(results_profiling_df.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
