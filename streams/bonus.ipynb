{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atomicwrites==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: attrs==19.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 2)) (19.1.0)\n",
      "Requirement already satisfied: certifi==2019.3.9 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: chardet==3.0.4 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: codecov==2.0.15 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 5)) (2.0.15)\n",
      "Requirement already satisfied: colorama==0.4.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: coverage==4.5.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 7)) (4.5.3)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: idna==2.8 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: imbalanced-learn==0.4.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: imblearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 11)) (0.0)\n",
      "Requirement already satisfied: importlib-metadata==0.17 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 12)) (0.17)\n",
      "Requirement already satisfied: joblib==0.13.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 13)) (0.13.2)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: matplotlib==3.1.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: more-itertools==7.0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 16)) (7.0.0)\n",
      "Requirement already satisfied: nltk==3.4.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 17)) (3.4.1)\n",
      "Requirement already satisfied: numpy==1.16.4 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 18)) (1.16.4)\n",
      "Requirement already satisfied: packaging==19.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 19)) (19.0)\n",
      "Requirement already satisfied: pandas==0.24.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 20)) (0.24.2)\n",
      "Requirement already satisfied: patsy==0.5.1 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (0.5.1)\n",
      "Requirement already satisfied: pluggy==0.12.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 22)) (0.12.0)\n",
      "Requirement already satisfied: py==1.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 23)) (1.8.0)\n",
      "Requirement already satisfied: pyparsing==2.4.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 24)) (2.4.0)\n",
      "Requirement already satisfied: pytest==4.6.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 25)) (4.6.0)\n",
      "Requirement already satisfied: pytest-cov==2.7.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 26)) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 27)) (2.8.0)\n",
      "Requirement already satisfied: pytz==2019.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 28)) (2019.1)\n",
      "Requirement already satisfied: requests==2.22.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 29)) (2.22.0)\n",
      "Requirement already satisfied: saxpy==1.0.1.dev167 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 30)) (1.0.1.dev167)\n",
      "Requirement already satisfied: scikit-learn==0.21.2 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 31)) (0.21.2)\n",
      "Requirement already satisfied: scipy==1.3.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 32)) (1.3.0)\n",
      "Requirement already satisfied: seaborn==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 33)) (0.9.0)\n",
      "Requirement already satisfied: six==1.12.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 34)) (1.12.0)\n",
      "Requirement already satisfied: sklearn==0.0 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 35)) (0.0)\n",
      "Requirement already satisfied: statsmodels==0.9.0 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 36)) (0.9.0)\n",
      "Requirement already satisfied: urllib3==1.25.3 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 37)) (1.25.3)\n",
      "Requirement already satisfied: wcwidth==0.1.7 in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 38)) (0.1.7)\n",
      "Requirement already satisfied: zipp==0.5.1 in c:\\users\\vasilis\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 39)) (0.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vasilis\\anaconda3\\lib\\site-packages (from kiwisolver==1.1.0->-r requirements.txt (line 14)) (40.6.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint, random, sample\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed lists for the creation of adversarial datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usage of these perturbation steps and types are inspired from Apruzzese, Giovanni, and Michele Colajanni. \n",
    "# \"Evading Botnet Detectors Based on Flows and Random Forest with Adversarial Samples.\" 2018 IEEE 17th International \n",
    "# Symposium on Network Computing and Applications (NCA). IEEE, 2018.\n",
    "perturbation_steps = {\n",
    "    1: {'packets': 1, 'bytes': 1},\n",
    "    2: {'packets': 2, 'bytes': 2},\n",
    "    3: {'packets': 5, 'bytes': 8},\n",
    "    4: {'packets': 10, 'bytes': 16},\n",
    "    5: {'packets': 15, 'bytes': 64},\n",
    "    6: {'packets': 20, 'bytes': 128},\n",
    "    7: {'packets': 30, 'bytes': 256},\n",
    "    8: {'packets': 50, 'bytes': 512},\n",
    "    9: {'packets': 100, 'bytes': 1024}\n",
    "}\n",
    "\n",
    "perturbation_types = {\n",
    "    1: ['packets'],\n",
    "    2: ['bytes'],\n",
    "    3: ['packets', 'bytes'],\n",
    "}\n",
    "\n",
    "# infected ips set used for the first version of adversarial dataset creation process\n",
    "infected_ips = ['147.32.84.155', '147.32.84.156', '147.32.84.157', '147.32.84.158', '147.32.84.159']\n",
    "\n",
    "# destination ips used for the first version of adversarial dataset creation process consisting of the infected and\n",
    "# normal hosts in the initial CTU scenario\n",
    "dest_ips = ['147.32.84.165', '147.32.84.191', '147.32.84.192', '147.32.84.193', '147.32.84.204',\n",
    "            '147.32.84.205', '147.32.84.206', '147.32.84.207', '147.32.84.208', '147.32.84.209',\n",
    "            '147.32.84.170', '147.32.84.134', '147.32.84.164', '147.32.87.36', '147.32.80.9', '147.32.87.11']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to create adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adversarial_v1(df, altered):\n",
    "    \"\"\"\n",
    "    Function that takes as input the initial malware dataset and adds some handcrafted flows with altered packets or\n",
    "    (and) bytes. The number of flows added are equal to 1% of the number of Botnet flows already existing. The\n",
    "    handcrafted flows have a new date (not existing in the initial dataset), a new source and destination ip addresses\n",
    "    while the rest features (apart from bytes and packets of course) are chosen randomly equal to some other flow in the\n",
    "    original dataset\n",
    "    :param df: the dataframe of the initial dataset\n",
    "    :param altered: the features to be altered\n",
    "    :return: the new adversarial dataset\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # first specify the number of adversarial flows to be constructed - currently 1% of the botnet data\n",
    "    n = int(0.01*len(df[df['label'] == 'Botnet']))\n",
    "\n",
    "    # generate n unique randoms indices from the range of the original indices\n",
    "    inds = sample(range(0, new_df.shape[0]-2), n)\n",
    "\n",
    "    # create the new unique dates for the new flows\n",
    "    new_dates = [new_df.index[ind] + random()*(new_df.index[ind+1]-new_df.index[ind]) for ind in inds]\n",
    "    new_flows = []\n",
    "    for ind, new_date in zip(inds, new_dates):\n",
    "        # create new flow\n",
    "        new_flow = [new_date]\n",
    "        new_flow += [new_df['duration'][ind]]  # duration\n",
    "        new_flow += ['TCP']  # and protocol is set to TCP\n",
    "        new_flow += [infected_ips[randint(0, 4)]]  # choose one of the new ips\n",
    "        new_flow += [new_df['src_port'][ind]]  # source port\n",
    "        new_flow += [dest_ips[randint(0, 15)]]  # choose one of the original botnet or normal ips\n",
    "        new_flow += [new_df['dst_port'][ind]]  # destination port\n",
    "        new_flow += [new_df['flags'][ind]]  # flags\n",
    "        new_flow += [new_df['tos'][ind]]  # and tos remain the same\n",
    "        new_flow += [new_df['packets'][ind] + 0 if 'packets' not in altered.keys() else altered['packets']]  # alter packets\n",
    "        new_flow += [new_df['bytes'][ind] + 0 if 'bytes' not in altered.keys() else altered['bytes']]  # alter bytes\n",
    "        new_flow += [new_df['flows'][ind]]  # flow remains the same\n",
    "        new_flow += ['Artificial']  # add the 'Artificial' label just for later plotting issues\n",
    "        new_flows += [new_flow]\n",
    "\n",
    "    # create the adversarial dataframe\n",
    "    column_names = ['date']+list(new_df.columns.values)\n",
    "    adv_df = pd.DataFrame(new_flows, columns=column_names)\n",
    "    adv_df = adv_df.set_index(adv_df.date)\n",
    "\n",
    "    # and concatenate it with the original dataset\n",
    "    fin_df = pd.concat([new_df, adv_df])\n",
    "    return fin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adversarial_v2(df, altered):\n",
    "    \"\"\"\n",
    "    The second version of adversarial dataset creation. In this version there are not new flows introduced. The botnet\n",
    "    flows are altered by adding packets or (and) bytes according to the perturbation types and steps specified as input\n",
    "    :param df: the dataframe of the initial dataset\n",
    "    :param altered: the features to be altered\n",
    "    :return: the new adversarial dataset\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    botnets = new_df[new_df['label'] == 'Botnet']  # keep the botnet flows\n",
    "    new_df = new_df[new_df['label'] != 'Botnet']  # and remove them from the original dataset\n",
    "\n",
    "    # alter the packets or (and) the bytes according ot the values of the altered dictionary\n",
    "    botnets['packets'] = botnets['packets'].apply(lambda z: z + 0 if 'packets' not in altered.keys() else altered['packets'])\n",
    "    botnets['bytes'] = botnets['bytes'].apply(lambda z: z + 0 if 'bytes' not in altered.keys() else altered['bytes'])\n",
    "\n",
    "    # and concatenate the new botnet flows with the original dataset with the original dataset\n",
    "    fin_df = pd.concat([new_df, botnets])\n",
    "    return fin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess the initial scenario 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data without the background are there, load them (again data from scenario 10 were used)\n",
    "data = pd.read_pickle('no_background_data.pkl')\n",
    "\n",
    "# resetting indices for data\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# parse packets and bytes as integers instead of strings\n",
    "data['packets'] = data['packets'].astype(int)\n",
    "data['bytes'] = data['bytes'].astype(int)\n",
    "\n",
    "# sort data by date just to be sure that flows are in chronological order\n",
    "data.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# set date as index in the dataframe\n",
    "data = data.set_index(data.date)\n",
    "data.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save the new adversarial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating perturbation type 1 with step 1...\n",
      "Creating perturbation type 1 with step 2...\n",
      "Creating perturbation type 1 with step 3...\n",
      "Creating perturbation type 1 with step 4...\n",
      "Creating perturbation type 1 with step 5...\n",
      "Creating perturbation type 1 with step 6...\n",
      "Creating perturbation type 1 with step 7...\n",
      "Creating perturbation type 1 with step 8...\n",
      "Creating perturbation type 1 with step 9...\n",
      "Creating perturbation type 2 with step 1...\n",
      "Creating perturbation type 2 with step 2...\n",
      "Creating perturbation type 2 with step 3...\n",
      "Creating perturbation type 2 with step 4...\n",
      "Creating perturbation type 2 with step 5...\n",
      "Creating perturbation type 2 with step 6...\n",
      "Creating perturbation type 2 with step 7...\n",
      "Creating perturbation type 2 with step 8...\n",
      "Creating perturbation type 2 with step 9...\n",
      "Creating perturbation type 3 with step 1...\n",
      "Creating perturbation type 3 with step 2...\n",
      "Creating perturbation type 3 with step 3...\n",
      "Creating perturbation type 3 with step 4...\n",
      "Creating perturbation type 3 with step 5...\n",
      "Creating perturbation type 3 with step 6...\n",
      "Creating perturbation type 3 with step 7...\n",
      "Creating perturbation type 3 with step 8...\n",
      "Creating perturbation type 3 with step 9...\n"
     ]
    }
   ],
   "source": [
    "# create all the adversarial datasets\n",
    "for types in perturbation_types.keys():\n",
    "    for step in perturbation_steps.keys():\n",
    "        print('Creating perturbation type %d with step %d...' % (types, step))\n",
    "        to_be_altered = {}\n",
    "        for type in perturbation_types[types]:\n",
    "            to_be_altered[type] = perturbation_steps[step][type]\n",
    "        # adv_df = make_adversarial_v1(data, to_be_altered)  # uncomment this line if you want to run the first\n",
    "                                                             # version of adversarial dataset creation\n",
    "        adv_df = make_adversarial_v2(data, to_be_altered)\n",
    "        adv_df.to_pickle('adversarial_examples/altered_%s_step_%d.pkl' % ('_'.join(perturbation_types[types]), step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
