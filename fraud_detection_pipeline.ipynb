{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.metrics as metrics\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \\\n",
    "    GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, NeighbourhoodCleaningRule, OneSidedSelection\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_timestamp(date_string):  \n",
    "    '''\n",
    "    Function coverting a time string to a float timestamp\n",
    "    '''\n",
    "    time_stamp = time.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "    return time.mktime(time_stamp)\n",
    "\n",
    "\n",
    "def create_initial_dataset():\n",
    "    '''\n",
    "    Function constructing the dataset from the initial csv file, keeping the categorical data\n",
    "    '''\n",
    "    src = 'data_for_student_case.csv'\n",
    "    ah = open(src, 'r')\n",
    "    x_labeled = []\n",
    "    data = []\n",
    "    conversion_dict = {'SEK': 0.09703, 'MXN': 0.04358, 'AUD': 0.63161, 'NZD': 0.58377, 'GBP': 1.13355} \n",
    "    ah.readline()  # skip first line\n",
    "    for line_ah in ah:\n",
    "        if line_ah.strip().split(',')[9] == 'Refused':  # skip Refused transactions\n",
    "            continue\n",
    "        if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "            continue\n",
    "        issuercountry = line_ah.strip().split(',')[2]  # country code\n",
    "        txvariantcode = line_ah.strip().split(',')[3]  # type of card: visa/master\n",
    "        issuer_id = float(line_ah.strip().split(',')[4])  # bin card issuer identifier\n",
    "        amount = float(line_ah.strip().split(',')[5])  # transaction amount in minor units\n",
    "        currencycode = line_ah.strip().split(',')[6]\n",
    "        amount = conversion_dict[currencycode] * amount  # currency conversion\n",
    "        shoppercountry = line_ah.strip().split(',')[7]  # country code\n",
    "        interaction = line_ah.strip().split(',')[8]  # online transaction or subscription\n",
    "        label  = 1 if line_ah.strip().split(',')[9] == 'Chargeback' else 0\n",
    "        verification = line_ah.strip().split(',')[10]  # shopper provide CVC code or not\n",
    "        cvcresponse = int(line_ah.strip().split(',')[11])  # 0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "        if cvcresponse > 2:\n",
    "            cvcresponse = 3\n",
    "        year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').year\n",
    "        month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').month\n",
    "        day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').day\n",
    "        creationdate = str(year_info) + '-' + str(month_info) + '-' + str(day_info)  # Date of transaction\n",
    "        creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])  # Date of transaction-time stamp\n",
    "        accountcode = line_ah.strip().split(',')[13]  # merchant’s webshop\n",
    "        mail_id = int(float(line_ah.strip().split(',')[14].replace('email', '')))  # mail\n",
    "        ip_id = int(float(line_ah.strip().split(',')[15].replace('ip', '')))  # ip\n",
    "        card_id = int(float(line_ah.strip().split(',')[16].replace('card', '')))  # card\n",
    "        data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                     shoppercountry, interaction, verification, cvcresponse, creationdate_stamp,\n",
    "                     accountcode, mail_id, ip_id, card_id, label, creationdate])  # add the interested features here\n",
    "    data = sorted(data, key=lambda k: k[-1])\n",
    "    for item in data:  # split data into x,y\n",
    "        x_labeled.append(item[0:-1])\n",
    "    df = pd.DataFrame(x_labeled)\n",
    "    df.columns = ['issuercountry', 'txvariantcode', 'issuer_id', 'amount', 'currencycode',\n",
    "                  'shoppercountry', 'interaction', 'verification', 'cvcresponse', 'creationdate_stamp',\n",
    "                  'accountcode', 'mail_id', 'ip_id', 'card_id', 'labels']  # column names of the dataset\n",
    "    df.to_csv('data_for_plots.csv')\n",
    "    \n",
    "    \n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Create the dataset for the bonus part from the original csv file\n",
    "    \"\"\"\n",
    "    src = 'data_for_student_case.csv'\n",
    "    ah = open(src, 'r')\n",
    "    x = []  # contains features\n",
    "    y = []  # contains labels\n",
    "    data = []\n",
    "    color = []\n",
    "    conversion_dict = {'SEK': 0.09703, 'MXN': 0.04358, 'AUD': 0.63161, 'NZD': 0.58377, 'GBP': 1.13355}\n",
    "    (issuercountry_set, txvariantcode_set, currencycode_set, shoppercountry_set, interaction_set,\n",
    "     verification_set, accountcode_set, mail_id_set, ip_id_set, card_id_set) = [set() for _ in range(10)]\n",
    "    (issuercountry_dict, txvariantcode_dict, currencycode_dict, shoppercountry_dict, interaction_dict,\n",
    "     verification_dict, accountcode_dict, mail_id_dict, ip_id_dict, card_id_dict) = [{} for _ in range(10)]\n",
    "\n",
    "    ah.readline()  # skip first line\n",
    "    for line_ah in ah:\n",
    "        if line_ah.strip().split(',')[9] == 'Refused':  # remove the row with 'refused' label, since it's uncertain about fraud\n",
    "            continue\n",
    "        if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "            continue\n",
    "        bookingdate = string_to_timestamp(line_ah.strip().split(',')[1])  # date reported flaud\n",
    "        issuercountry = line_ah.strip().split(',')[2]  # country code\n",
    "        issuercountry_set.add(issuercountry)\n",
    "        txvariantcode = line_ah.strip().split(',')[3]  # type of card: visa/master\n",
    "        txvariantcode_set.add(txvariantcode)\n",
    "        issuer_id = float(line_ah.strip().split(',')[4])  # bin card issuer identifier\n",
    "        amount = float(line_ah.strip().split(',')[5])  # transaction amount in minor units\n",
    "        currencycode = line_ah.strip().split(',')[6]\n",
    "        currencycode_set.add(currencycode)\n",
    "        amount = conversion_dict[currencycode] * amount  # currency conversion\n",
    "        shoppercountry = line_ah.strip().split(',')[7]  # country code\n",
    "        shoppercountry_set.add(shoppercountry)\n",
    "        interaction = line_ah.strip().split(',')[8]  # online transaction or subscription\n",
    "        interaction_set.add(interaction)\n",
    "        if line_ah.strip().split(',')[9] == 'Chargeback':\n",
    "            label = 1  # label fraud\n",
    "        else:\n",
    "            label = 0  # label save\n",
    "        verification = line_ah.strip().split(',')[10]  # shopper provide CVC code or not\n",
    "        verification_set.add(verification)\n",
    "        cvcresponse = int(line_ah.strip().split(',')[11])  # 0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "        if cvcresponse > 2:\n",
    "            cvcresponse = 3\n",
    "        crd = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S')\n",
    "        year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').year\n",
    "        month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').month\n",
    "        day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').day\n",
    "        creationdate = str(year_info) + '-' + str(month_info) + '-' + str(day_info)  # Date of transaction\n",
    "        creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])  # Date of transaction-time stamp\n",
    "        accountcode = line_ah.strip().split(',')[13]  # merchant’s webshop\n",
    "        accountcode_set.add(accountcode)\n",
    "        mail_id = int(float(line_ah.strip().split(',')[14].replace('email', '')))  # mail\n",
    "        mail_id_set.add(mail_id)\n",
    "        ip_id = int(float(line_ah.strip().split(',')[15].replace('ip', '')))  # ip\n",
    "        ip_id_set.add(ip_id)\n",
    "        card_id = int(float(line_ah.strip().split(',')[16].replace('card', '')))  # card\n",
    "        card_id_set.add(card_id)\n",
    "        data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                     shoppercountry, interaction, verification, cvcresponse, crd,\n",
    "                     accountcode, mail_id, ip_id, card_id, label, creationdate])  # add the interested features here\n",
    "\n",
    "    data = sorted(data, key=lambda k: k[9])  # data sorted according to transaction-time stamp\n",
    "\n",
    "    for item in data:  # split data into x,y\n",
    "        x.append(item[0:-2])\n",
    "        y.append(item[-2])\n",
    "\n",
    "    # map number to each categorial feature\n",
    "    for item in list(issuercountry_set):\n",
    "        issuercountry_dict[item] = list(issuercountry_set).index(item)\n",
    "    for item in list(txvariantcode_set):\n",
    "        txvariantcode_dict[item] = list(txvariantcode_set).index(item)\n",
    "    for item in list(currencycode_set):\n",
    "        currencycode_dict[item] = list(currencycode_set).index(item)\n",
    "    for item in list(shoppercountry_set):\n",
    "        shoppercountry_dict[item] = list(shoppercountry_set).index(item)\n",
    "    for item in list(interaction_set):\n",
    "        interaction_dict[item] = list(interaction_set).index(item)\n",
    "    for item in list(verification_set):\n",
    "        verification_dict[item] = list(verification_set).index(item)\n",
    "    for item in list(accountcode_set):\n",
    "        accountcode_dict[item] = list(accountcode_set).index(item)\n",
    "\n",
    "    # modify categorial feature to number in data set\n",
    "    for item in x:\n",
    "        item[0] = issuercountry_dict[item[0]]\n",
    "        item[1] = txvariantcode_dict[item[1]]\n",
    "        item[4] = currencycode_dict[item[4]]\n",
    "        item[5] = shoppercountry_dict[item[5]]\n",
    "        item[6] = interaction_dict[item[6]]\n",
    "        item[7] = verification_dict[item[7]]\n",
    "        item[10] = accountcode_dict[item[10]]\n",
    "\n",
    "    # The \"original_data_for_aggr.csv\" numeric dataset\n",
    "    # used to compute the aggregated features is created\n",
    "    des = 'original_data_for_aggr.csv'\n",
    "    ch_dfa = open(des, 'w')\n",
    "\n",
    "    ch_dfa.write(\n",
    "        'issuercountry,txvariantcode,issuer_id,amount,currencycode,shoppercountry,interaction,verification,cvcresponse,'\n",
    "        'creationdate,accountcode,mail_id,ip_id,card_id,label')\n",
    "    ch_dfa.write('\\n')\n",
    "\n",
    "    sentence = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            sentence.append(str(x[i][j]))\n",
    "        sentence.append(str(y[i]))\n",
    "        ch_dfa.write(','.join(sentence))\n",
    "        ch_dfa.write('\\n')\n",
    "        sentence = []\n",
    "        ch_dfa.flush()\n",
    "    \n",
    "    \n",
    "def make_boxplot(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the accountcode-to-amount feature relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"accountcode\", y=\"amount\", hue=\"labels\", data=data,\n",
    "                palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"])\n",
    "    plt.xlabel(\"Merchant's webshop\")\n",
    "    plt.ylabel(\"Amount in euros\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_accountcode_amount.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def make_boxplot_money(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-amount relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"amount\", data=data,\n",
    "                palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"Amount in euros\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_amount.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def make_barplot(data):\n",
    "    '''\n",
    "    Function for the visualization task - Barplot about the percentage of cvcresponce code per class \n",
    "    '''\n",
    "    cvc_counts = (data.groupby(['labels'])['cvcresponse'].value_counts(normalize=True).rename('percentage').mul(100)\n",
    "                         .reset_index().sort_values('cvcresponse'))\n",
    "    ax = sns.barplot(x='cvcresponse', y='percentage', data=cvc_counts, hue='labels',\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']})\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"], loc='upper right')\n",
    "    plt.xlabel(\"CVC code\")\n",
    "    plt.ylabel(\"Percentage of occurrences\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/barplot_cvc.png')\n",
    "    print('barplot created')\n",
    "\n",
    "\n",
    "def make_barplot_issued(data):\n",
    "    '''\n",
    "    Function for the visualization task - Barplot about the percentage of issuercountry code per class \n",
    "    '''\n",
    "    cvc_counts = (data.groupby(['labels'])['issuercountry'].value_counts(normalize=True).rename('percentage').mul(100)\n",
    "                         .reset_index())\n",
    "    ax = sns.barplot(x='issuercountry', y='percentage', data=cvc_counts, hue='labels',\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']})\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"], loc='upper right')\n",
    "    ax.set(xlim=(-0.5, 15.5))\n",
    "    plt.xlabel(\"Issuer Country\")\n",
    "    plt.ylabel(\"Percentage of occurrences\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/barplot_issuer.png')\n",
    "    print('barplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_card_type(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the amount-to-card type feature relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"amount\", y=\"txvariantcode\", hue=\"labels\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"])\n",
    "    plt.xlabel(\"Amount in euros\")\n",
    "    plt.ylabel(\"Type of card\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_card_type.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_issuer_id(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-issuer id relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"issuer_id\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"Card issuer identifier\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_issuer_id.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_ip(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-ip id relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"ip_id\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"IP address\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_ip.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def plot_roc(fpr, tpr, roc_auc, fpr_smote, tpr_smote, roc_auc_smote, clf_name):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the ROC curve for both the SMOTEd and unSMOTEd classifier\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.title('{} - Receiver Operating Characteristic'.format(clf_name))\n",
    "    plt.plot(fpr, tpr, 'r', label='AUC unSMOTEd = %0.2f' % roc_auc)\n",
    "    plt.plot(fpr_smote, tpr_smote, 'g', label='AUC SMOTEd = %0.2f' % roc_auc_smote)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'b--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "#     plt.savefig('imbalance_plots/{}_ROC.png'.format(clf_name), bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, clf_name, classes, smote, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the confusion matrix of a classifier \n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:  # check if the normalized confusion matrix is to be plotted\n",
    "            title = '{} - Normalized confusion matrix'.format(clf_name+smote)\n",
    "        else:\n",
    "            title = '{} - Confusion matrix, without normalization'.format(clf_name+smote)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "#     fig.savefig('imbalance_plots/{}_confusion.png'.format(clf_name+smote))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_prec_rec(precision, recall, precision_smote, recall_smote):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the Precision-Recall curve for both the SMOTEd and unSMOTEd classifier\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.title('{} - Precision-recall curve'.format(clf_name))\n",
    "    plt.plot(recall, precision, 'r', label='Precision-recall UnSMOTEd')\n",
    "    plt.plot(recall_smote, precision_smote, 'g', label='Precision-recall SMOTEd')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "#     plt.savefig('imbalance_plots/{}_Prec_Rec.png'.format(clf_name), bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "def plot_decision_tree(clf):\n",
    "    '''\n",
    "    Function for the classification task - Plots the structure of the trained decision tree\n",
    "    '''\n",
    "    features = np.array(['issuercountry', 'txvariantcode', 'issuer_id', 'amount', 'currencycode',\n",
    "                         'shoppercountry', 'interaction', 'verification', 'cvcresponse', 'creationdate_stamp',\n",
    "                         'accountcode', 'mail_id', 'ip_id', 'card_id'])\n",
    "    fearure_nums = [0, 4, 5, 8, 10, 12]  # features to be used\n",
    "    graph = Source(export_graphviz(clf, out_file=None, max_depth=3, feature_names=features[fearure_nums],\n",
    "                    class_names=['benign', 'fraudulent'], filled=True, rounded=True, special_characters=True,\n",
    "                    proportion=False, precision=2))\n",
    "    png_bytes = graph.pipe(format='png')\n",
    "    with open('dtree.png', 'wb') as f:\n",
    "        f.write(png_bytes)\n",
    "    \n",
    "\n",
    "def make_clf_SMOTE(usx, usy, clf, clf_name, normalize=False, smoted=False):\n",
    "    '''\n",
    "    Function for the imbalance task - Trains and tests the classifier clf using 10-fold cross-validation\n",
    "    If normalize flag is True then the data are being normalised\n",
    "    If smoted flag is True then SMOTE is used to oversample the minority class \n",
    "    '''\n",
    "    print('----------{}----------'.format(clf_name))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    total_y_test = []\n",
    "    total_y_prob = []\n",
    "    total_y_pred = []\n",
    "    skf = StratifiedKFold(n_splits=10)  # 10-fold stratified cross validation\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "\n",
    "        if smoted:  # SMOTE applied\n",
    "            sm = SMOTE(sampling_strategy=0.5)\n",
    "            x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "        if normalize: # normalization applied\n",
    "            scaler = RobustScaler().fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "        clf.fit(x_train, y_train)  # training of the classifier\n",
    "        y_predict = clf.predict(x_test)  # testing the classifier\n",
    "        y_proba = clf.predict_proba(x_test)  # class-wise probabilities of each test sample\n",
    "        # needed for roc curve\n",
    "        total_y_test += list(y_test)\n",
    "        total_y_prob += list(y_proba[:, 1])\n",
    "        total_y_pred += list(y_predict)\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_predict[i] and y_proba[i, 1] <= 0.65:   # adding more \"cost\" to the misclassification in the fraudulent class\n",
    "                y_predict[i] = 0\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] and y_predict[i]:\n",
    "                totalTP += 1\n",
    "            if not y_test[i] and y_predict[i]:\n",
    "                totalFP += 1\n",
    "            if y_test[i] and not y_predict[i]:\n",
    "                totalFN += 1\n",
    "            if not y_test[i] and not y_predict[i]:\n",
    "                totalTN += 1\n",
    "\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))\n",
    "\n",
    "    total_y_test = np.array(total_y_test)\n",
    "    total_y_prob = np.array(total_y_prob)\n",
    "    total_y_pred = np.array(total_y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(total_y_test, total_y_prob)  # create FPR ans TPR values for the ROC curve plotting\n",
    "    precision, recall, _ = precision_recall_curve(total_y_test, total_y_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)  # Area under curve calculation\n",
    "    if smoted:\n",
    "        plot_confusion_matrix(total_y_test, total_y_pred, clf_name, ['benign', 'fraudulent'], ' SMOTEd',\n",
    "                            normalize=True, title=None, cmap=plt.cm.Blues)\n",
    "    else:\n",
    "        plot_confusion_matrix(total_y_test, total_y_pred, clf_name, ['benign', 'fraudulent'], '',\n",
    "                              normalize=True, title=None, cmap=plt.cm.Blues)\n",
    "    return fpr, tpr, roc_auc, precision, recall\n",
    "\n",
    "\n",
    "def make_clf(usx, usy, clf, clf_name, sampling, normalize=False):\n",
    "    '''\n",
    "    Function for the classification task - Trains and tests the classifier clf using 10-fold cross-validation\n",
    "    If normalize flag is True then the data are being normalised\n",
    "    The sampling parameter sets the type of sampling to be used\n",
    "    '''\n",
    "    print('----------{} with {}----------'.format(clf_name, sampling))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    plot_ind = randint(0, 9)\n",
    "    j = 0\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "        \n",
    "        # sampling applied\n",
    "        if sampling == 'SMOTE':\n",
    "            x_train, y_train = SMOTE(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'ADASYN':\n",
    "            x_train, y_train = ADASYN(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'ENN':\n",
    "            x_train, y_train = EditedNearestNeighbours().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'Tomek':\n",
    "            x_train, y_train = TomekLinks().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'SMOTETomek':\n",
    "            x_train, y_train = SMOTETomek(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'SMOTEENN':\n",
    "            x_train, y_train = SMOTEENN(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'NCR':\n",
    "            x_train, y_train = NeighbourhoodCleaningRule().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'OSS':\n",
    "            x_train, y_train = OneSidedSelection().fit_resample(x_train, y_train)\n",
    "\n",
    "        if normalize:\n",
    "            scaler = StandardScaler().fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        # Decision tree plot for the white-box algorithm needs\n",
    "        # if plot_ind == j and clf_name == 'DecisionTreeClassifier':\n",
    "        #     plot_decision_tree(clf)\n",
    "\n",
    "        y_predict = clf.predict(x_test)\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] and y_predict[i]:\n",
    "                totalTP += 1\n",
    "            if not y_test[i] and y_predict[i]:\n",
    "                totalFP += 1\n",
    "            if y_test[i] and not y_predict[i]:\n",
    "                totalFN += 1\n",
    "            if not y_test[i] and not y_predict[i]:\n",
    "                totalTN += 1\n",
    "        j += 1\n",
    "\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))\n",
    "    \n",
    "    \n",
    "def make_clf_bonus(usx, usy, clf, clf_name, strategy='SMOTE', normalize=False, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Makes classification with the given parameters and print the results\n",
    "    :param usx: features\n",
    "    :param usy: labels\n",
    "    :param clf: classifier\n",
    "    :param clf_name: name of the classifier\n",
    "    :param strategy: sampling strategy to be used\n",
    "    :param normalize: boolean to decide whether to normilize or not\n",
    "    :param cutoff: cutoff value for the classification threshold\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('----------{0} with {1}----------'.format(clf_name, strategy))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    total_y_test = []\n",
    "    total_y_prob = []\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "\n",
    "        # select sampling strategy\n",
    "        if strategy == 'SMOTE':\n",
    "            sm = SMOTE(sampling_strategy=0.5, n_jobs=-1)\n",
    "            x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "        elif strategy == 'ADASYN':\n",
    "            ad = ADASYN(n_jobs=-1)\n",
    "            x_train, y_train = ad.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "        elif strategy == 'ENN':\n",
    "            en = EditedNearestNeighbours(n_jobs=-1)\n",
    "            x_train, y_train = en.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "        elif strategy == 'TL':\n",
    "            tl = TomekLinks(sampling_strategy='auto', n_jobs=-1)\n",
    "            x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "        elif strategy == 'OSS':\n",
    "            oss = OneSidedSelection(n_jobs=-1)\n",
    "            x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "        elif strategy == 'SMOTEK':\n",
    "            sm = SMOTETomek(smote=SMOTE(sampling_strategy=0.5, n_jobs=-1), tomek=TomekLinks(n_jobs=-1))\n",
    "            x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))  # use this with threshold\n",
    "        elif strategy == 'SMOTEEN':\n",
    "            smoteenn = SMOTEENN(smote=SMOTE(sampling_strategy=0.5, n_jobs=-1), enn=EditedNearestNeighbours(n_jobs=-1))\n",
    "            x_train, y_train = smoteenn.fit_resample(x_train, y_train)\n",
    "            print('Resampled dataset shape %s' % Counter(y_train))\n",
    "\n",
    "        # normalize data if needed\n",
    "        if normalize:\n",
    "            scaler = StandardScaler().fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "        # fit classifier\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        # predict labels and their probaility\n",
    "        y_predict = clf.predict(x_test)\n",
    "        y_predict_proba = clf.predict_proba(x_test)\n",
    "\n",
    "        total_y_test += list(y_test)\n",
    "        total_y_prob += list(y_predict_proba[:, 1])\n",
    "\n",
    "        # modify the threshold for the two classes\n",
    "        if cutoff < 0.5:\n",
    "            for i in range(len(y_predict)):\n",
    "                if y_predict[i] == 0 and y_predict_proba[i, 1] >= cutoff:\n",
    "                    y_predict[i] = 1\n",
    "        elif cutoff > 0.5:\n",
    "            for i in range(len(y_predict)):\n",
    "                if y_predict[i] == 1 and y_predict_proba[i, 1] <= cutoff:\n",
    "                    y_predict[i] = 0\n",
    "\n",
    "        TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] == 1 and y_predict[i] == 1:\n",
    "                TP += 1\n",
    "            if y_test[i] == 0 and y_predict[i] == 1:\n",
    "                FP += 1\n",
    "            if y_test[i] == 1 and y_predict[i] == 0:\n",
    "                FN += 1\n",
    "            if y_test[i] == 0 and y_predict[i] == 0:\n",
    "                TN += 1\n",
    "        totalFN += FN\n",
    "        totalFP += FP\n",
    "        totalTN += TN\n",
    "        totalTP += TP\n",
    "\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))\n",
    "    \n",
    "    \n",
    "# Below there are the functions that are used for the aggregated features\n",
    "def custom_aggr(cols, d):\n",
    "    l = 0\n",
    "    ids = cols['card_id']\n",
    "    creationdate = cols['creationdate']\n",
    "    for id, crd in zip(ids, creationdate):\n",
    "        l += len([v for v in d[id] if 0 <= (crd - v).days <= 30]) - 1\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_am_per_trans(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    ids = cols['card_id']\n",
    "    creationdate = cols['creationdate']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    for id, crd, em in zip(ids, creationdate, euroamount):\n",
    "        l += np.mean([v for i, v in enumerate(d_am[id]) if 0 <= (crd - d_crd[id][i]).days <= 30])\n",
    "    return int(round(l))\n",
    "\n",
    "\n",
    "def custom_aggr_avg_over_3_mon(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    ids = cols['card_id']\n",
    "    creationdate = cols['creationdate']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    for id, crd, em in zip(ids, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[id]) if 0 <= (crd - d_crd[id][i]).days <= 90])\n",
    "    l = int(round(l / 12))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_avg_daily_over_month(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    ids = cols['card_id']\n",
    "    creationdate = cols['creationdate']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    for id, crd, em in zip(ids, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[id]) if 0 <= (crd - d_crd[id][i]).days <= 30])\n",
    "    l = int(round(l / 30))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_amount_merchant_over_month(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, acc, crd, em in zip(card_id, accountcode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[(cid, acc)]) if 0 <= (crd - d_crd[(cid, acc)][i]).days <= 30]) - em\n",
    "    l = int(round(l / 30))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_diff_merchant_over_month(cols, d_crd):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, acc, crd, em in zip(card_id, accountcode, creationdate, euroamount):\n",
    "        l += len([v for v in d_crd[(cid, acc)] if 0 <= (crd - v).days <= 30]) - 1\n",
    "    return int(round(l))\n",
    "\n",
    "\n",
    "def custom_aggr_amount_merchant_over_3_months(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, acc, crd, em in zip(card_id, accountcode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[(cid, acc)]) if 0 <= (crd - d_crd[(cid, acc)][i]).days <= 90]) - em\n",
    "    l = int(round(l / 12))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_amount_same_day(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, crd, em in zip(card_id, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[cid]) if crd.date() == d_crd[cid][i].date()]) - em\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_number_same_day(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, crd in zip(card_id, creationdate):\n",
    "        l += len([v for i, v in enumerate(d_am[cid]) if crd.date() == d_crd[cid][i].date()]) - 1\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_amount_same_merchant(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    d_am = np.array(d_am)\n",
    "    for acc, crd, em in zip(accountcode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[acc]) if 0 <= (crd - d_crd[acc][i]).days <= 30]) - em\n",
    "        # l += np.sum([(d_am[list(map(lambda x: 0 <= (crd - x).days <= 30, d_crd[acc]))])]) - em\n",
    "    l = int(round(l / 30))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_num_same_merchant(cols, d_crd):\n",
    "    l = 0\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for acc, crd, em in zip(accountcode, creationdate, euroamount):\n",
    "        l += len([v for v in d_crd[acc] if 0 <= (crd - v).days <= 30]) - 1\n",
    "    return int(round(l-1))\n",
    "\n",
    "\n",
    "def custom_aggr_amount_currency_over_month(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    currencycode = cols['currencycode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, cur, crd, em in zip(card_id, currencycode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[(cid, cur)]) if 0 <= (crd - d_crd[(cid, cur)][i]).days <= 30]) - em\n",
    "    l = int(round(l / 30))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_diff_currency_over_month(cols, d_crd):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    currencycode = cols['currencycode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, cur, crd, em in zip(card_id, currencycode, creationdate, euroamount):\n",
    "        l += len([v for v in d_crd[(cid, cur)] if 0 <= (crd - v).days <= 30]) - 1\n",
    "    return int(round(l))\n",
    "\n",
    "\n",
    "def custom_aggr_amount_country_over_month(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    shoppercountrycode = cols['shoppercountrycode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, shc, crd, em in zip(card_id, shoppercountrycode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[(cid, shc)]) if 0 <= (crd - d_crd[(cid, shc)][i]).days <= 30]) - em\n",
    "    l = int(round(l / 30))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_diff_country_over_month(cols, d_crd):\n",
    "    l = 0\n",
    "    card_id = cols['card_id']\n",
    "    euroamount = cols['EuroAmount']\n",
    "    shoppercountrycode = cols['shoppercountrycode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for cid, shc, crd, em in zip(card_id, shoppercountrycode, creationdate, euroamount):\n",
    "        l += len([v for v in d_crd[(cid, shc)] if 0 <= (crd - v).days <= 30]) - 1\n",
    "    return int(round(l))\n",
    "\n",
    "\n",
    "def custom_aggr_amount_merchant_type_over_3_months(cols, d_crd, d_am):\n",
    "    l = 0\n",
    "    euroamount = cols['EuroAmount']\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for acc, crd, em in zip(accountcode, creationdate, euroamount):\n",
    "        l += np.sum([v for i, v in enumerate(d_am[acc]) if 0 <= (crd - d_crd[acc][i]).days <= 90]) - em\n",
    "    l = int(round(l / 12))\n",
    "    return l\n",
    "\n",
    "\n",
    "def custom_aggr_diff_merchant_type_over_3_months(cols, d_crd):\n",
    "    l = 0\n",
    "    accountcode = cols['accountcode']\n",
    "    creationdate = cols['creationdate']\n",
    "    for acc, crd in zip(accountcode, creationdate):\n",
    "        l += len([v for v in d_crd[acc] if 0 <= (crd - v).days <= 90]) - 1\n",
    "    l = int(round(l))\n",
    "    return l\n",
    "\n",
    "\n",
    "def aggregate_data(data):\n",
    "    \"\"\"\n",
    "    Computes the aggregated features\n",
    "    :param data: original dataset\n",
    "    :return: dataset with aggregated features\n",
    "    \"\"\"\n",
    "    gen = data[['card_id', 'creationdate', 'EuroAmount', 'txvariantcode', 'accountcode', 'shoppercountrycode', 'currencycode']]\n",
    "\n",
    "    # number of transactions on the last month\n",
    "    trans_over_month = data[['card_id', 'creationdate']]\n",
    "    d1 = gen.groupby('card_id')['creationdate'].apply(list).to_dict()\n",
    "    trans_over_month = trans_over_month.groupby(['card_id', 'creationdate'], as_index=False)[['card_id', 'creationdate']].agg(custom_aggr, d1).to_frame('trans_past_month')\n",
    "    data = pd.merge(data, trans_over_month, how='inner', on=['card_id', 'creationdate'])\n",
    "\n",
    "    # amount spent on the transactions of the last month\n",
    "    amount_per_trans = data[['card_id', 'EuroAmount', 'creationdate']]\n",
    "    d2 = gen.groupby('card_id')['EuroAmount'].apply(list).to_dict()\n",
    "    amount_per_trans = amount_per_trans.groupby(['card_id', 'EuroAmount', 'creationdate'], as_index=False)['card_id', 'EuroAmount', 'creationdate'].agg(custom_aggr_am_per_trans, d1, d2).to_frame('avg_amount_per_trans')\n",
    "    data = pd.merge(data, amount_per_trans, how='inner', on=['card_id', 'creationdate'])\n",
    "\n",
    "    # average amount spent over last 3 months\n",
    "    avg_over_3_mon = data[['card_id', 'EuroAmount', 'creationdate']]\n",
    "    avg_over_3_mon = avg_over_3_mon.groupby(['card_id', 'EuroAmount', 'creationdate'], as_index=False)[['card_id', 'EuroAmount', 'creationdate']].agg(custom_aggr_avg_over_3_mon, d1, d2).to_frame('avg_over_3_mon')\n",
    "    data = pd.merge(data, avg_over_3_mon, how='inner', on=['card_id', 'creationdate'])\n",
    "\n",
    "    # average daily amount spent over last month\n",
    "    avg_daily_over_month = data[['card_id', 'EuroAmount', 'creationdate']]\n",
    "    avg_daily_over_month = avg_daily_over_month.groupby(['card_id', 'EuroAmount', 'creationdate'], as_index=False)['card_id', 'EuroAmount', 'creationdate'].agg(custom_aggr_avg_daily_over_month, d1, d2).to_frame('avg_daily_over_month')\n",
    "    data = pd.merge(data, avg_daily_over_month, how='inner', on=['card_id', 'creationdate'])\n",
    "\n",
    "    # average daily amount spent over last month\n",
    "    amount_merchant_over_month = data[['card_id', 'accountcode', 'creationdate','EuroAmount']]\n",
    "    d3 = gen.groupby(['card_id', 'accountcode'])['creationdate'].apply(list).to_dict()\n",
    "    d4 = gen.groupby(['card_id', 'accountcode'])['EuroAmount'].apply(list).to_dict()\n",
    "    amount_merchant_over_month = amount_merchant_over_month.groupby(['card_id', 'accountcode', 'creationdate', 'EuroAmount'], as_index=False)['card_id', 'accountcode', 'creationdate', 'EuroAmount'].agg(custom_aggr_amount_merchant_over_month, d3, d4).to_frame('amount_merchant_over_month')\n",
    "    data = pd.merge(data, amount_merchant_over_month, how='inner', on=['card_id', 'accountcode', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "    # number of different merchant over the last month\n",
    "    diff_merchant_over_month = data[['card_id', 'accountcode', 'creationdate','EuroAmount']]\n",
    "    diff_merchant_over_month = diff_merchant_over_month.groupby(['card_id', 'accountcode', 'creationdate','EuroAmount'], as_index=False)['card_id', 'accountcode', 'creationdate','EuroAmount'].agg(custom_aggr_diff_merchant_over_month, d3).to_frame('diff_merchant_over_month')\n",
    "    data = pd.merge(data, diff_merchant_over_month, how='inner', on=['card_id', 'accountcode', 'creationdate','EuroAmount'])\n",
    "\n",
    "    # amount spent the last 3 month one this merchant\n",
    "    amount_merchant_over_3_months = data[['card_id', 'EuroAmount', 'accountcode', 'creationdate']]\n",
    "    amount_merchant_over_3_months = \\\n",
    "    amount_merchant_over_3_months.groupby(['card_id', 'EuroAmount', 'accountcode', 'creationdate'], as_index=False).agg(custom_aggr_amount_merchant_over_3_months, d3, d4).to_frame('amount_merchant_over_3_months')\n",
    "    data = pd.merge(data, amount_merchant_over_3_months, how='inner', on=['card_id', 'EuroAmount', 'accountcode', 'creationdate'])\n",
    "\n",
    "    # amount spent by this card the same day as the one of the transaction\n",
    "    amount_same_day = data[['card_id', 'creationdate', 'EuroAmount']]\n",
    "    amount_same_day = amount_same_day.groupby(['card_id', 'creationdate', 'EuroAmount'], as_index=False)['card_id', 'creationdate', 'EuroAmount'].agg(custom_aggr_amount_same_day,d1, d2).to_frame('amount_same_day')\n",
    "    data = pd.merge(data, amount_same_day, how='inner', on=['card_id', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "    # number of trancactions done the same day by this card\n",
    "    number_same_day = data[['card_id', 'creationdate']]\n",
    "    number_same_day = number_same_day.groupby(['card_id', 'creationdate'], as_index=False)['card_id', 'creationdate'].agg(custom_aggr_number_same_day, d1, d2).to_frame('number_same_day')\n",
    "    data = pd.merge(data, number_same_day, how='inner', on=['card_id', 'creationdate'])\n",
    "\n",
    "    # amount spent on this currency over last month by this card\n",
    "    amount_currency_over_month = data[['card_id', 'currencycode', 'creationdate', 'EuroAmount']]\n",
    "    d7 = gen.groupby(['card_id', 'currencycode'])['creationdate'].apply(list).to_dict()\n",
    "    d8 = gen.groupby(['card_id', 'currencycode'])['EuroAmount'].apply(list).to_dict()\n",
    "    amount_currency_over_month = amount_currency_over_month.groupby(['card_id', 'currencycode', 'creationdate', 'EuroAmount'], as_index=False)['card_id', 'currencycode', 'creationdate', 'EuroAmount'].agg(custom_aggr_amount_currency_over_month, d7,d8).to_frame('amount_currency_over_month')\n",
    "    data = pd.merge(data, amount_currency_over_month, how='inner', on=['card_id', 'currencycode', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "\n",
    "    # different currencies used by this card the last month\n",
    "    diff_currency_over_month = data[['card_id', 'currencycode', 'creationdate', 'EuroAmount']]\n",
    "    diff_currency_over_month = diff_currency_over_month.groupby(['card_id', 'currencycode', 'creationdate', 'EuroAmount'], as_index=False)['card_id', 'currencycode', 'creationdate', 'EuroAmount'].agg(custom_aggr_diff_currency_over_month,d7).to_frame('diff_currency_over_month')\n",
    "    data = pd.merge(data, diff_currency_over_month, how='inner', on=['card_id', 'currencycode', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "\n",
    "    # amount spent on this country over the last month\n",
    "    amount_country_over_month = data[['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount']]\n",
    "    d9 = gen.groupby(['card_id', 'shoppercountrycode'])['creationdate'].apply(list).to_dict()\n",
    "    d10 = gen.groupby(['card_id', 'shoppercountrycode'])['EuroAmount'].apply(list).to_dict()\n",
    "    amount_country_over_month = amount_country_over_month.groupby(['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'],as_index=False)['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'].agg(custom_aggr_amount_country_over_month,d9, d10).to_frame('amount_country_over_month')\n",
    "    data = pd.merge(data, amount_country_over_month, how='inner', on=['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "\n",
    "    # number of different countries of transactions the last month\n",
    "    diff_country_over_month = data[['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount']]\n",
    "    diff_country_over_month = diff_country_over_month.groupby(['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'], as_index=False)['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'].agg(custom_aggr_diff_country_over_month,d9).to_frame('diff_country_over_month')\n",
    "    data = pd.merge(data, diff_country_over_month, how='inner', on=['card_id', 'shoppercountrycode', 'creationdate', 'EuroAmount'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_encoding(data, column_name, threshold):\n",
    "    count = dict(data[column_name].value_counts())\n",
    "    mapping = {}\n",
    "    for id in count.keys():\n",
    "        if count[id] > threshold:\n",
    "            mapping[id] = id\n",
    "        else:\n",
    "            mapping[id] = 'dk'\n",
    "    data[column_name] = data[column_name].map(mapping)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_initial_dataset()  # to be used if the \"data_for_plots.csv\" is not already created\n",
    "data = pd.read_csv('data_for_plots.csv')\n",
    "plt.figure()\n",
    "make_boxplot(data)\n",
    "plt.figure()\n",
    "make_barplot(data)\n",
    "plt.figure()\n",
    "make_boxplot_money(data)\n",
    "plt.figure()\n",
    "make_barplot_issued(data)\n",
    "plt.figure(figsize=(13, 8))\n",
    "make_boxplot_card_type(data)\n",
    "# plt.figure()\n",
    "# make_boxplot_issuer_id(data)\n",
    "plt.figure()\n",
    "make_boxplot_ip(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'data_for_student_case.csv'\n",
    "ah = open(src, 'r')\n",
    "x = []  # contains features\n",
    "y = []  # contains labels\n",
    "data = []\n",
    "color = []\n",
    "conversion_dict = {'SEK': 0.09703, 'MXN': 0.04358, 'AUD': 0.63161, 'NZD': 0.58377, 'GBP': 1.13355}\n",
    "(issuercountry_set, txvariantcode_set, currencycode_set, shoppercountry_set, interaction_set,\n",
    " verification_set, accountcode_set, mail_id_set, ip_id_set, card_id_set) = [set() for _ in range(10)]\n",
    "(issuercountry_dict, txvariantcode_dict, currencycode_dict, shoppercountry_dict, interaction_dict,\n",
    " verification_dict, accountcode_dict, mail_id_dict, ip_id_dict, card_id_dict) = [{} for _ in range(10)]\n",
    "ah.readline()  # skip first line\n",
    "for line_ah in ah:\n",
    "    if line_ah.strip().split(',')[9] == 'Refused':  # remove the row with 'refused' label, since it's uncertain about fraud\n",
    "        continue\n",
    "    if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "        continue\n",
    "    bookingdate = string_to_timestamp(line_ah.strip().split(',')[1])  # date reported flaud\n",
    "    issuercountry = line_ah.strip().split(',')[2]  # country code\n",
    "    issuercountry_set.add(issuercountry)\n",
    "    txvariantcode = line_ah.strip().split(',')[3]  # type of card: visa/master\n",
    "    txvariantcode_set.add(txvariantcode)\n",
    "    issuer_id = float(line_ah.strip().split(',')[4])  # bin card issuer identifier\n",
    "    amount = float(line_ah.strip().split(',')[5])  # transaction amount in minor units\n",
    "    currencycode = line_ah.strip().split(',')[6]\n",
    "    currencycode_set.add(currencycode)\n",
    "    amount = conversion_dict[currencycode] * amount  # currency conversion\n",
    "    shoppercountry = line_ah.strip().split(',')[7]  # country code\n",
    "    shoppercountry_set.add(shoppercountry)\n",
    "    interaction = line_ah.strip().split(',')[8]  # online transaction or subscription\n",
    "    interaction_set.add(interaction)\n",
    "    if line_ah.strip().split(',')[9] == 'Chargeback':\n",
    "        label = 1  # label fraud\n",
    "    else:\n",
    "        label = 0  # label save\n",
    "    verification = line_ah.strip().split(',')[10]  # shopper provide CVC code or not\n",
    "    verification_set.add(verification)\n",
    "    cvcresponse = int(line_ah.strip().split(',')[11])  # 0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "    if cvcresponse > 2:\n",
    "        cvcresponse = 3\n",
    "    year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').year\n",
    "    month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').month\n",
    "    day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').day\n",
    "    creationdate = str(year_info) + '-' + str(month_info) + '-' + str(day_info)  # Date of transaction\n",
    "    creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])  # Date of transaction-time stamp\n",
    "    accountcode = line_ah.strip().split(',')[13]  # merchant’s webshop\n",
    "    accountcode_set.add(accountcode)\n",
    "    mail_id = int(float(line_ah.strip().split(',')[14].replace('email', '')))  # mail\n",
    "    mail_id_set.add(mail_id)\n",
    "    ip_id = int(float(line_ah.strip().split(',')[15].replace('ip', '')))  # ip\n",
    "    ip_id_set.add(ip_id)\n",
    "    card_id = int(float(line_ah.strip().split(',')[16].replace('card', '')))  # card\n",
    "    card_id_set.add(card_id)\n",
    "    data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                 shoppercountry, interaction, verification, cvcresponse, creationdate_stamp,\n",
    "                 accountcode, mail_id, ip_id, card_id, label, creationdate])  # add the interested features here\n",
    "\n",
    "data = sorted(data, key=lambda k: k[9])  # data sorted according to transaction-time stamp\n",
    "\n",
    "for item in data:  # split data into x,y\n",
    "    x.append(item[0:-2])\n",
    "    y.append(item[-2])\n",
    "    \n",
    "# map number to each categorial feature\n",
    "for item in list(issuercountry_set):\n",
    "    issuercountry_dict[item] = list(issuercountry_set).index(item)\n",
    "for item in list(txvariantcode_set):\n",
    "    txvariantcode_dict[item] = list(txvariantcode_set).index(item)\n",
    "for item in list(currencycode_set):\n",
    "    currencycode_dict[item] = list(currencycode_set).index(item)\n",
    "for item in list(shoppercountry_set):\n",
    "    shoppercountry_dict[item] = list(shoppercountry_set).index(item)\n",
    "for item in list(interaction_set):\n",
    "    interaction_dict[item] = list(interaction_set).index(item)\n",
    "for item in list(verification_set):\n",
    "    verification_dict[item] = list(verification_set).index(item)\n",
    "for item in list(accountcode_set):\n",
    "    accountcode_dict[item] = list(accountcode_set).index(item)\n",
    "# print(len(list(card_id_set)))\n",
    "\n",
    "# modify categorial feature to number in data set\n",
    "for item in x:\n",
    "    item[0] = issuercountry_dict[item[0]]\n",
    "    item[1] = txvariantcode_dict[item[1]]\n",
    "    item[4] = currencycode_dict[item[4]]\n",
    "    item[5] = shoppercountry_dict[item[5]]\n",
    "    item[6] = interaction_dict[item[6]]\n",
    "    item[7] = verification_dict[item[7]]\n",
    "    item[10] = accountcode_dict[item[10]]\n",
    "\n",
    "# The \"original_data.csv\" numeric dataset is created\n",
    "des = 'original_data.csv'\n",
    "ch_dfa = open(des, 'w')\n",
    "\n",
    "ch_dfa.write(\n",
    "    'issuercountry, txvariantcode, issuer_id, amount, currencycode, shoppercountry, interaction, '\n",
    "    'verification, cvcresponse, creationdate_stamp, accountcode, mail_id, ip_id, card_id, label')\n",
    "ch_dfa.write('\\n')\n",
    "\n",
    "sentence = []\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[i])):\n",
    "        sentence.append(str(x[i][j]))\n",
    "    sentence.append(str(y[i]))\n",
    "    ch_dfa.write(','.join(sentence))\n",
    "    ch_dfa.write('\\n')\n",
    "    sentence = []\n",
    "    ch_dfa.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'original_data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "x = np.delete(x, [2, 4, 5, 6, 7, 9, 11, 13], 1)  # specific features are kept - selection done mainly according to the \n",
    "                                                 #  relationships identified in the visualization part\n",
    "# dictionary with the tested classifiers\n",
    "clfs = {'KNeighborsClassifier': neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='auto', weights='distance')\n",
    "        , 'LogisticRegression': LogisticRegression(solver='newton-cg')\n",
    "        , 'NaiveBayes': GaussianNB()\n",
    "        , 'AdaBoostClassifier': AdaBoostClassifier(n_estimators=50)\n",
    "        , 'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n",
    "        , 'ExtraTreesClassifier': ExtraTreesClassifier(n_estimators=100)\n",
    "        , 'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=100)\n",
    "        , 'VotingClassifier': VotingClassifier(estimators=[\n",
    "                ('knn', neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree', weights='distance')),\n",
    "                ('lr', LogisticRegression(solver='newton-cg')),\n",
    "                ('gnb', GaussianNB())\n",
    "                ], voting='soft')\n",
    "        }\n",
    "for clf_name, clf in clfs.items():  # check all the classifiers both with and without SMOTE \n",
    "    usx = np.copy(x)\n",
    "    usy = np.copy(y)\n",
    "    if clf_name == 'LogisticRegression':\n",
    "        fpr, tpr, roc_auc, precision, recall = make_clf_SMOTE(usx, usy, clf, clf_name, normalize=True)\n",
    "        fpr_smote, tpr_smote, roc_auc_smote, precision_smote, recall_smote = \\\n",
    "            make_clf_SMOTE(usx, usy, clf, clf_name, normalize=True, smoted=True)\n",
    "    else:\n",
    "        fpr, tpr, roc_auc, precision, recall = make_clf_SMOTE(usx, usy, clf, clf_name)\n",
    "        fpr_smote, tpr_smote, roc_auc_smote, precision_smote, recall_smote = \\\n",
    "            make_clf_SMOTE(usx, usy, clf, clf_name, smoted=True)\n",
    "    plot_roc(fpr, tpr, roc_auc, fpr_smote, tpr_smote, roc_auc_smote, clf_name)\n",
    "    # plot_prec_rec(precision, recall, precision_smote, recall_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'original_data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "x = np.delete(x, [1, 2, 3, 6, 7, 9, 11, 13], 1)  # specific features are kept \n",
    "\n",
    "# dictionaries with the two classifiers to be tested\n",
    "clfs = {'DecisionTreeClassifier': DecisionTreeClassifier(criterion='entropy', class_weight='balanced')   \n",
    "        , 'RandomForestClassifier': RandomForestClassifier(n_estimators=50, criterion='entropy',\n",
    "                                                           class_weight='balanced')  \n",
    "        }\n",
    "for smlp in ['SMOTE', 'ADASYN', 'Tomek', 'OSS', 'ENN', 'SMOTETomek', 'SMOTEENN']:  # check different types of sampling\n",
    "    for clf_name, clf in clfs.items():\n",
    "        usx = np.copy(x)\n",
    "        usy = np.copy(y)\n",
    "        make_clf(usx, usy, clf, clf_name, smlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to create the dataset. I will take some time !!!\n",
    "# create_dataset()\n",
    "# filename = 'original_data_for_aggr.csv'\n",
    "# data = pd.read_csv(filename)\n",
    "# data = data.rename(columns={'amount': 'EuroAmount', 'shoppercountry': 'shoppercountrycode'})\n",
    "#\n",
    "# # add aggregated features\n",
    "# data = aggregate_data(deepcopy(data))\n",
    "\n",
    "# use this to load the aggregated features instead of calculating them again\n",
    "with open('data_numerical.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "# keep only useful features\n",
    "data = data.drop(['txvariantcode', 'EuroAmount', 'interaction','verification', 'creationdate', 'mail_id'], axis=1)\n",
    "\n",
    "# convert bin to integer\n",
    "data['bin'] = data.apply(lambda r: int(round(r['issuer_id'])), axis=1)\n",
    "\n",
    "\n",
    "# encode data\n",
    "data = data_encoding(deepcopy(data), 'issuer_id', 3)\n",
    "data = data_encoding(deepcopy(data), 'card_id', 10)\n",
    "\n",
    "# one-hot encoding\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# get features and labels\n",
    "x = data.drop('label', axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "\n",
    "# apply pca\n",
    "print('Applying PCA')\n",
    "pca = PCA(n_components=100)\n",
    "x = pca.fit_transform(x)\n",
    "print('PCA done')\n",
    "\n",
    "# define the classifiers used and their hyperparameters\n",
    "clfs = {'RandomForestClassifier': RandomForestClassifier(n_estimators=50, criterion='entropy',\n",
    "                                                         class_weight='balanced', n_jobs=-1),\n",
    "        'DecisionTreeClassifier': DecisionTreeClassifier(criterion='entropy', class_weight='balanced')}\n",
    "\n",
    "# make classification\n",
    "for clf_name, clf in clfs.items():\n",
    "    usx = np.copy(x)\n",
    "    usy = np.copy(y)\n",
    "    make_clf_bonus(usx, usy, clf, clf_name, strategy='SMOTE', cutoff=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
