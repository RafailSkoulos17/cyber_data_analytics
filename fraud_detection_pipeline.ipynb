{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \\\n",
    "    GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, NeighbourhoodCleaningRule, OneSidedSelection\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_timestamp(date_string):  \n",
    "    '''\n",
    "    Function coverting a time string to a float timestamp\n",
    "    '''\n",
    "    time_stamp = time.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "    return time.mktime(time_stamp)\n",
    "\n",
    "\n",
    "def create_initial_dataset():\n",
    "    '''\n",
    "    Function constructing the dataset from the initial csv file, keeping the categorical data\n",
    "    '''\n",
    "    src = 'data_for_student_case.csv'\n",
    "    ah = open(src, 'r')\n",
    "    x_labeled = []\n",
    "    data = []\n",
    "    conversion_dict = {'SEK': 0.09703, 'MXN': 0.04358, 'AUD': 0.63161, 'NZD': 0.58377, 'GBP': 1.13355} \n",
    "    ah.readline()  # skip first line\n",
    "    for line_ah in ah:\n",
    "        if line_ah.strip().split(',')[9] == 'Refused':  # skip Refused transactions\n",
    "            continue\n",
    "        if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "            continue\n",
    "        issuercountry = line_ah.strip().split(',')[2]  # country code\n",
    "        txvariantcode = line_ah.strip().split(',')[3]  # type of card: visa/master\n",
    "        issuer_id = float(line_ah.strip().split(',')[4])  # bin card issuer identifier\n",
    "        amount = float(line_ah.strip().split(',')[5])  # transaction amount in minor units\n",
    "        currencycode = line_ah.strip().split(',')[6]\n",
    "        amount = conversion_dict[currencycode] * amount  # currency conversion\n",
    "        shoppercountry = line_ah.strip().split(',')[7]  # country code\n",
    "        interaction = line_ah.strip().split(',')[8]  # online transaction or subscription\n",
    "        label  = 1 if line_ah.strip().split(',')[9] == 'Chargeback' else 0\n",
    "        verification = line_ah.strip().split(',')[10]  # shopper provide CVC code or not\n",
    "        cvcresponse = int(line_ah.strip().split(',')[11])  # 0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "        if cvcresponse > 2:\n",
    "            cvcresponse = 3\n",
    "        year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').year\n",
    "        month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').month\n",
    "        day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').day\n",
    "        creationdate = str(year_info) + '-' + str(month_info) + '-' + str(day_info)  # Date of transaction\n",
    "        creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])  # Date of transaction-time stamp\n",
    "        accountcode = line_ah.strip().split(',')[13]  # merchantâ€™s webshop\n",
    "        mail_id = int(float(line_ah.strip().split(',')[14].replace('email', '')))  # mail\n",
    "        ip_id = int(float(line_ah.strip().split(',')[15].replace('ip', '')))  # ip\n",
    "        card_id = int(float(line_ah.strip().split(',')[16].replace('card', '')))  # card\n",
    "        data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                     shoppercountry, interaction, verification, cvcresponse, creationdate_stamp,\n",
    "                     accountcode, mail_id, ip_id, card_id, label, creationdate])  # add the interested features here\n",
    "    data = sorted(data, key=lambda k: k[-1])\n",
    "    for item in data:  # split data into x,y\n",
    "        x_labeled.append(item[0:-1])\n",
    "    df = pd.DataFrame(x_labeled)\n",
    "    df.columns = ['issuercountry', 'txvariantcode', 'issuer_id', 'amount', 'currencycode',\n",
    "                  'shoppercountry', 'interaction', 'verification', 'cvcresponse', 'creationdate_stamp',\n",
    "                  'accountcode', 'mail_id', 'ip_id', 'card_id', 'labels']  # column names of the dataset\n",
    "    df.to_csv('data_for_plots.csv')\n",
    "    \n",
    "    \n",
    "def make_boxplot(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the accountcode-to-amount feature relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"accountcode\", y=\"amount\", hue=\"labels\", data=data,\n",
    "                palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"])\n",
    "    plt.xlabel(\"Merchant's webshop\")\n",
    "    plt.ylabel(\"Amount in euros\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_accountcode_amount.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def make_boxplot_money(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-amount relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"amount\", data=data,\n",
    "                palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"Amount in euros\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_amount.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def make_barplot(data):\n",
    "    '''\n",
    "    Function for the visualization task - Barplot about the percentage of cvcresponce code per class \n",
    "    '''\n",
    "    cvc_counts = (data.groupby(['labels'])['cvcresponse'].value_counts(normalize=True).rename('percentage').mul(100)\n",
    "                         .reset_index().sort_values('cvcresponse'))\n",
    "    ax = sns.barplot(x='cvcresponse', y='percentage', data=cvc_counts, hue='labels',\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']})\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"], loc='upper right')\n",
    "    plt.xlabel(\"CVC code\")\n",
    "    plt.ylabel(\"Percentage of occurrences\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/barplot_cvc.png')\n",
    "    print('barplot created')\n",
    "\n",
    "\n",
    "def make_barplot_issued(data):\n",
    "    '''\n",
    "    Function for the visualization task - Barplot about the percentage of issuercountry code per class \n",
    "    '''\n",
    "    cvc_counts = (data.groupby(['labels'])['issuercountry'].value_counts(normalize=True).rename('percentage').mul(100)\n",
    "                         .reset_index())\n",
    "    ax = sns.barplot(x='issuercountry', y='percentage', data=cvc_counts, hue='labels',\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']})\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"], loc='upper right')\n",
    "    ax.set(xlim=(-0.5, 15.5))\n",
    "    plt.xlabel(\"Issuer Country\")\n",
    "    plt.ylabel(\"Percentage of occurrences\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/barplot_issuer.png')\n",
    "    print('barplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_card_type(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the amount-to-card type feature relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"amount\", y=\"txvariantcode\", hue=\"labels\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"benign\", \"fraudulent\"])\n",
    "    plt.xlabel(\"Amount in euros\")\n",
    "    plt.ylabel(\"Type of card\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_card_type.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_issuer_id(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-issuer id relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"issuer_id\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"Card issuer identifier\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_issuer_id.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "\n",
    "def make_boxplot_ip(data):\n",
    "    '''\n",
    "    Function for the visualization task - Boxplot about the labels-to-ip id relationship\n",
    "    '''\n",
    "    ax = sns.boxplot(x=\"labels\", y=\"ip_id\", data=data,\n",
    "                     palette={0: mcolors.TABLEAU_COLORS['tab:blue'], 1: mcolors.TABLEAU_COLORS['tab:red']}, sym=\"\")\n",
    "    ax.set_xticklabels(['benign', 'fraudulent'])\n",
    "    plt.ylabel(\"IP address\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     plt.savefig('plots/boxplot_labels_ip.png')\n",
    "    print('boxplot created')\n",
    "\n",
    "    \n",
    "def plot_roc(fpr, tpr, roc_auc, fpr_smote, tpr_smote, roc_auc_smote, clf_name):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the ROC curve for both the SMOTEd and unSMOTEd classifier\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.title('{} - Receiver Operating Characteristic'.format(clf_name))\n",
    "    plt.plot(fpr, tpr, 'r', label='AUC unSMOTEd = %0.2f' % roc_auc)\n",
    "    plt.plot(fpr_smote, tpr_smote, 'g', label='AUC SMOTEd = %0.2f' % roc_auc_smote)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'b--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "#     plt.savefig('imbalance_plots/{}_ROC.png'.format(clf_name), bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, clf_name, classes, smote, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the confusion matrix of a classifier \n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:  # check if the normalized confusion matrix is to be plotted\n",
    "            title = '{} - Normalized confusion matrix'.format(clf_name+smote)\n",
    "        else:\n",
    "            title = '{} - Confusion matrix, without normalization'.format(clf_name+smote)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "#     fig.savefig('imbalance_plots/{}_confusion.png'.format(clf_name+smote))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_prec_rec(precision, recall, precision_smote, recall_smote):\n",
    "    '''\n",
    "    Function for the imbalance task - Plots the Precision-Recall curve for both the SMOTEd and unSMOTEd classifier\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.title('{} - Precision-recall curve'.format(clf_name))\n",
    "    plt.plot(recall, precision, 'r', label='Precision-recall UnSMOTEd')\n",
    "    plt.plot(recall_smote, precision_smote, 'g', label='Precision-recall SMOTEd')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "#     plt.savefig('imbalance_plots/{}_Prec_Rec.png'.format(clf_name), bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "def plot_decision_tree(clf):\n",
    "    '''\n",
    "    Function for the classification task - Plots the structure of the trained decision tree\n",
    "    '''\n",
    "    features = np.array(['issuercountry', 'txvariantcode', 'issuer_id', 'amount', 'currencycode',\n",
    "                         'shoppercountry', 'interaction', 'verification', 'cvcresponse', 'creationdate_stamp',\n",
    "                         'accountcode', 'mail_id', 'ip_id', 'card_id'])\n",
    "    fearure_nums = [0, 4, 5, 8, 10, 12]  # features to be used\n",
    "    graph = Source(export_graphviz(clf, out_file=None, max_depth=3, feature_names=features[fearure_nums],\n",
    "                    class_names=['benign', 'fraudulent'], filled=True, rounded=True, special_characters=True,\n",
    "                    proportion=False, precision=2))\n",
    "    png_bytes = graph.pipe(format='png')\n",
    "    with open('dtree.png', 'wb') as f:\n",
    "        f.write(png_bytes)\n",
    "    \n",
    "\n",
    "def make_clf_SMOTE(usx, usy, clf, clf_name, normalize=False, smoted=False):\n",
    "    '''\n",
    "    Function for the imbalance task - Trains and tests the classifier clf using 10-fold cross-validation\n",
    "    If normalize flag is True then the data are being normalised\n",
    "    If smoted flag is True then SMOTE is used to oversample the minority class \n",
    "    '''\n",
    "    print('----------{}----------'.format(clf_name))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    total_y_test = []\n",
    "    total_y_prob = []\n",
    "    total_y_pred = []\n",
    "    skf = StratifiedKFold(n_splits=10)  # 10-fold stratified cross validation\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "\n",
    "        if smoted:  # SMOTE applied\n",
    "            sm = SMOTE(sampling_strategy=0.5)\n",
    "            x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "        if normalize: # normalization applied\n",
    "            scaler = RobustScaler().fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "        clf.fit(x_train, y_train)  # training of the classifier\n",
    "        y_predict = clf.predict(x_test)  # testing the classifier\n",
    "        y_proba = clf.predict_proba(x_test)  # class-wise probabilities of each test sample\n",
    "        # needed for roc curve\n",
    "        total_y_test += list(y_test)\n",
    "        total_y_prob += list(y_proba[:, 1])\n",
    "        total_y_pred += list(y_predict)\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_predict[i] and y_proba[i, 1] <= 0.65:   # adding more \"cost\" to the misclassification in the fraudulent class\n",
    "                y_predict[i] = 0\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] and y_predict[i]:\n",
    "                totalTP += 1\n",
    "            if not y_test[i] and y_predict[i]:\n",
    "                totalFP += 1\n",
    "            if y_test[i] and not y_predict[i]:\n",
    "                totalFN += 1\n",
    "            if not y_test[i] and not y_predict[i]:\n",
    "                totalTN += 1\n",
    "\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))\n",
    "\n",
    "    total_y_test = np.array(total_y_test)\n",
    "    total_y_prob = np.array(total_y_prob)\n",
    "    total_y_pred = np.array(total_y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(total_y_test, total_y_prob)  # create FPR ans TPR values for the ROC curve plotting\n",
    "    precision, recall, _ = precision_recall_curve(total_y_test, total_y_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)  # Area under curve calculation\n",
    "    if smoted:\n",
    "        plot_confusion_matrix(total_y_test, total_y_pred, clf_name, ['benign', 'fraudulent'], ' SMOTEd',\n",
    "                            normalize=True, title=None, cmap=plt.cm.Blues)\n",
    "    else:\n",
    "        plot_confusion_matrix(total_y_test, total_y_pred, clf_name, ['benign', 'fraudulent'], '',\n",
    "                              normalize=True, title=None, cmap=plt.cm.Blues)\n",
    "    return fpr, tpr, roc_auc, precision, recall\n",
    "\n",
    "\n",
    "def make_clf(usx, usy, clf, clf_name, sampling, normalize=False):\n",
    "    '''\n",
    "    Function for the classification task - Trains and tests the classifier clf using 10-fold cross-validation\n",
    "    If normalize flag is True then the data are being normalised\n",
    "    The sampling parameter sets the type of sampling to be used\n",
    "    '''\n",
    "    print('----------{} with {}----------'.format(clf_name, sampling))\n",
    "    totalTP, totalFP, totalFN, totalTN = 0, 0, 0, 0\n",
    "    plot_ind = randint(0, 9)\n",
    "    j = 0\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    for train_index, test_index in skf.split(usx, usy):\n",
    "        x_train, x_test = usx[train_index], usx[test_index]\n",
    "        y_train, y_test = usy[train_index], usy[test_index]\n",
    "        \n",
    "        # sampling applied\n",
    "        if sampling == 'SMOTE':\n",
    "            x_train, y_train = SMOTE(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'ADASYN':\n",
    "            x_train, y_train = ADASYN(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'ENN':\n",
    "            x_train, y_train = EditedNearestNeighbours().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'Tomek':\n",
    "            x_train, y_train = TomekLinks().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'SMOTETomek':\n",
    "            x_train, y_train = SMOTETomek(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'SMOTEENN':\n",
    "            x_train, y_train = SMOTEENN(sampling_strategy=0.3).fit_resample(x_train, y_train)\n",
    "        elif sampling == 'NCR':\n",
    "            x_train, y_train = NeighbourhoodCleaningRule().fit_resample(x_train, y_train)\n",
    "        elif sampling == 'OSS':\n",
    "            x_train, y_train = OneSidedSelection().fit_resample(x_train, y_train)\n",
    "\n",
    "        if normalize:\n",
    "            scaler = StandardScaler().fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        # Decision tree plot for the white-box algorithm needs\n",
    "        # if plot_ind == j and clf_name == 'DecisionTreeClassifier':\n",
    "        #     plot_decision_tree(clf)\n",
    "\n",
    "        y_predict = clf.predict(x_test)\n",
    "\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i] and y_predict[i]:\n",
    "                totalTP += 1\n",
    "            if not y_test[i] and y_predict[i]:\n",
    "                totalFP += 1\n",
    "            if y_test[i] and not y_predict[i]:\n",
    "                totalFN += 1\n",
    "            if not y_test[i] and not y_predict[i]:\n",
    "                totalTN += 1\n",
    "        j += 1\n",
    "\n",
    "    print('TOTAL TP: ' + str(totalTP))\n",
    "    print('TOTAL FP: ' + str(totalFP))\n",
    "    print('TOTAL FN: ' + str(totalFN))\n",
    "    print('TOTAL TN: ' + str(totalTN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_initial_dataset()  # to be used if the \"data_for_plots.csv\" is not already created\n",
    "data = pd.read_csv('data_for_plots.csv')\n",
    "plt.figure()\n",
    "make_boxplot(data)\n",
    "plt.figure()\n",
    "make_barplot(data)\n",
    "plt.figure()\n",
    "make_boxplot_money(data)\n",
    "plt.figure()\n",
    "make_barplot_issued(data)\n",
    "plt.figure(figsize=(13, 8))\n",
    "make_boxplot_card_type(data)\n",
    "# plt.figure()\n",
    "# make_boxplot_issuer_id(data)\n",
    "plt.figure()\n",
    "make_boxplot_ip(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'data_for_student_case.csv'\n",
    "ah = open(src, 'r')\n",
    "x = []  # contains features\n",
    "y = []  # contains labels\n",
    "data = []\n",
    "color = []\n",
    "conversion_dict = {'SEK': 0.09703, 'MXN': 0.04358, 'AUD': 0.63161, 'NZD': 0.58377, 'GBP': 1.13355}\n",
    "(issuercountry_set, txvariantcode_set, currencycode_set, shoppercountry_set, interaction_set,\n",
    " verification_set, accountcode_set, mail_id_set, ip_id_set, card_id_set) = [set() for _ in range(10)]\n",
    "(issuercountry_dict, txvariantcode_dict, currencycode_dict, shoppercountry_dict, interaction_dict,\n",
    " verification_dict, accountcode_dict, mail_id_dict, ip_id_dict, card_id_dict) = [{} for _ in range(10)]\n",
    "ah.readline()  # skip first line\n",
    "for line_ah in ah:\n",
    "    if line_ah.strip().split(',')[9] == 'Refused':  # remove the row with 'refused' label, since it's uncertain about fraud\n",
    "        continue\n",
    "    if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "        continue\n",
    "    bookingdate = string_to_timestamp(line_ah.strip().split(',')[1])  # date reported flaud\n",
    "    issuercountry = line_ah.strip().split(',')[2]  # country code\n",
    "    issuercountry_set.add(issuercountry)\n",
    "    txvariantcode = line_ah.strip().split(',')[3]  # type of card: visa/master\n",
    "    txvariantcode_set.add(txvariantcode)\n",
    "    issuer_id = float(line_ah.strip().split(',')[4])  # bin card issuer identifier\n",
    "    amount = float(line_ah.strip().split(',')[5])  # transaction amount in minor units\n",
    "    currencycode = line_ah.strip().split(',')[6]\n",
    "    currencycode_set.add(currencycode)\n",
    "    amount = conversion_dict[currencycode] * amount  # currency conversion\n",
    "    shoppercountry = line_ah.strip().split(',')[7]  # country code\n",
    "    shoppercountry_set.add(shoppercountry)\n",
    "    interaction = line_ah.strip().split(',')[8]  # online transaction or subscription\n",
    "    interaction_set.add(interaction)\n",
    "    if line_ah.strip().split(',')[9] == 'Chargeback':\n",
    "        label = 1  # label fraud\n",
    "    else:\n",
    "        label = 0  # label save\n",
    "    verification = line_ah.strip().split(',')[10]  # shopper provide CVC code or not\n",
    "    verification_set.add(verification)\n",
    "    cvcresponse = int(line_ah.strip().split(',')[11])  # 0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "    if cvcresponse > 2:\n",
    "        cvcresponse = 3\n",
    "    year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').year\n",
    "    month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').month\n",
    "    day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12], '%Y-%m-%d %H:%M:%S').day\n",
    "    creationdate = str(year_info) + '-' + str(month_info) + '-' + str(day_info)  # Date of transaction\n",
    "    creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])  # Date of transaction-time stamp\n",
    "    accountcode = line_ah.strip().split(',')[13]  # merchantâ€™s webshop\n",
    "    accountcode_set.add(accountcode)\n",
    "    mail_id = int(float(line_ah.strip().split(',')[14].replace('email', '')))  # mail\n",
    "    mail_id_set.add(mail_id)\n",
    "    ip_id = int(float(line_ah.strip().split(',')[15].replace('ip', '')))  # ip\n",
    "    ip_id_set.add(ip_id)\n",
    "    card_id = int(float(line_ah.strip().split(',')[16].replace('card', '')))  # card\n",
    "    card_id_set.add(card_id)\n",
    "    data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                 shoppercountry, interaction, verification, cvcresponse, creationdate_stamp,\n",
    "                 accountcode, mail_id, ip_id, card_id, label, creationdate])  # add the interested features here\n",
    "\n",
    "data = sorted(data, key=lambda k: k[9])  # data sorted according to transaction-time stamp\n",
    "\n",
    "for item in data:  # split data into x,y\n",
    "    x.append(item[0:-2])\n",
    "    y.append(item[-2])\n",
    "    \n",
    "# map number to each categorial feature\n",
    "for item in list(issuercountry_set):\n",
    "    issuercountry_dict[item] = list(issuercountry_set).index(item)\n",
    "for item in list(txvariantcode_set):\n",
    "    txvariantcode_dict[item] = list(txvariantcode_set).index(item)\n",
    "for item in list(currencycode_set):\n",
    "    currencycode_dict[item] = list(currencycode_set).index(item)\n",
    "for item in list(shoppercountry_set):\n",
    "    shoppercountry_dict[item] = list(shoppercountry_set).index(item)\n",
    "for item in list(interaction_set):\n",
    "    interaction_dict[item] = list(interaction_set).index(item)\n",
    "for item in list(verification_set):\n",
    "    verification_dict[item] = list(verification_set).index(item)\n",
    "for item in list(accountcode_set):\n",
    "    accountcode_dict[item] = list(accountcode_set).index(item)\n",
    "# print(len(list(card_id_set)))\n",
    "\n",
    "# modify categorial feature to number in data set\n",
    "for item in x:\n",
    "    item[0] = issuercountry_dict[item[0]]\n",
    "    item[1] = txvariantcode_dict[item[1]]\n",
    "    item[4] = currencycode_dict[item[4]]\n",
    "    item[5] = shoppercountry_dict[item[5]]\n",
    "    item[6] = interaction_dict[item[6]]\n",
    "    item[7] = verification_dict[item[7]]\n",
    "    item[10] = accountcode_dict[item[10]]\n",
    "\n",
    "# The \"original_data.csv\" numeric dataset is created\n",
    "des = 'original_data.csv'\n",
    "ch_dfa = open(des, 'w')\n",
    "\n",
    "ch_dfa.write(\n",
    "    'issuercountry, txvariantcode, issuer_id, amount, currencycode, shoppercountry, interaction, '\n",
    "    'verification, cvcresponse, creationdate_stamp, accountcode, mail_id, ip_id, card_id, label')\n",
    "ch_dfa.write('\\n')\n",
    "\n",
    "sentence = []\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[i])):\n",
    "        sentence.append(str(x[i][j]))\n",
    "    sentence.append(str(y[i]))\n",
    "    ch_dfa.write(','.join(sentence))\n",
    "    ch_dfa.write('\\n')\n",
    "    sentence = []\n",
    "    ch_dfa.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'original_data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "x = np.delete(x, [2, 4, 5, 6, 7, 9, 11, 13], 1)  # specific features are kept - selection done mainly according to the \n",
    "                                                 #  relationships identified in the visualization part\n",
    "# dictionary with the tested classifiers\n",
    "clfs = {'KNeighborsClassifier': neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='auto', weights='distance')\n",
    "        , 'LogisticRegression': LogisticRegression(solver='newton-cg')\n",
    "        , 'NaiveBayes': GaussianNB()\n",
    "        , 'AdaBoostClassifier': AdaBoostClassifier(n_estimators=50)\n",
    "        , 'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n",
    "        , 'ExtraTreesClassifier': ExtraTreesClassifier(n_estimators=100)\n",
    "        , 'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=100)\n",
    "        , 'VotingClassifier': VotingClassifier(estimators=[\n",
    "                ('knn', neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree', weights='distance')),\n",
    "                ('lr', LogisticRegression(solver='newton-cg')),\n",
    "                ('gnb', GaussianNB())\n",
    "                ], voting='soft')\n",
    "        }\n",
    "for clf_name, clf in clfs.items():  # check all the classifiers both with and without SMOTE \n",
    "    usx = np.copy(x)\n",
    "    usy = np.copy(y)\n",
    "    if clf_name == 'LogisticRegression':\n",
    "        fpr, tpr, roc_auc, precision, recall = make_clf_SMOTE(usx, usy, clf, clf_name, normalize=True)\n",
    "        fpr_smote, tpr_smote, roc_auc_smote, precision_smote, recall_smote = \\\n",
    "            make_clf_SMOTE(usx, usy, clf, clf_name, normalize=True, smoted=True)\n",
    "    else:\n",
    "        fpr, tpr, roc_auc, precision, recall = make_clf_SMOTE(usx, usy, clf, clf_name)\n",
    "        fpr_smote, tpr_smote, roc_auc_smote, precision_smote, recall_smote = \\\n",
    "            make_clf_SMOTE(usx, usy, clf, clf_name, smoted=True)\n",
    "    plot_roc(fpr, tpr, roc_auc, fpr_smote, tpr_smote, roc_auc_smote, clf_name)\n",
    "    # plot_prec_rec(precision, recall, precision_smote, recall_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'original_data.csv'\n",
    "data = pd.read_csv(filename)\n",
    "x = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "x = np.delete(x, [1, 2, 3, 6, 7, 9, 11, 13], 1)  # specific features are kept \n",
    "\n",
    "# dictionaries with the two classifiers to be tested\n",
    "clfs = {'DecisionTreeClassifier': DecisionTreeClassifier(criterion='entropy', class_weight='balanced')   \n",
    "        , 'RandomForestClassifier': RandomForestClassifier(n_estimators=50, criterion='entropy',\n",
    "                                                           class_weight='balanced')  \n",
    "        }\n",
    "for smlp in ['SMOTE', 'ADASYN', 'Tomek', 'OSS', 'ENN', 'SMOTETomek', 'SMOTEENN']:  # check different types of sampling\n",
    "    for clf_name, clf in clfs.items():\n",
    "        usx = np.copy(x)\n",
    "        usy = np.copy(y)\n",
    "        make_clf(usx, usy, clf, clf_name, smlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
